{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Representing Documents as Numbers\n",
    "## NLP for the Creative Industries \n",
    "### Louis McCallum 2021 \n",
    "(with minor edits by Teresa Pelinski 2022, Rebecca Fiebrink 2023)\n",
    "\n",
    "Hopefully, you have followed the Git tutorial and have managed to update your repo and pull in this new notebook! \n",
    "\n",
    "If we want to take a document or set of documents, and use them with machine learning techniques, or other mathematical operation to uncover some new information about them, we can't just use the text and characters. We need a new representation, and that needs to be numerical. This week we will be taking our first steps to representing collections of documents as numbers. \n",
    "\n",
    "- Tokenisation \n",
    "- Bags of Words \n",
    "- n-Grams\n",
    "\n",
    "### Building a Vocabulary \n",
    "\n",
    "The first step to getting a new, better representation of a text document is splitting it up into its constituent parts. We call these **tokens** and deciding _what a token is_ is an important choice. \n",
    "\n",
    "### Basic Tokenisation - ``str.split()``\n",
    "What is the simplest way split a **String** into **tokens**? \n",
    "\n",
    "Introducing the `str.split()` function. Here we take a long string (multiple words) and split it into separate words (or **tokens**) based on spaces. \n",
    "\n",
    "We have previously seen **functions** that take a String as an argument and return a new value (e.g., `foo(bar)` where `foo` is a function and `bar` is a string). In this case, the concept is broadly similar, apart from we **call the function on the string itself** (e.g., `bar.foo()`).\n",
    "\n",
    "What gets returned is a **List**, containing our split string. We store it in the variable ``tokens``, and print it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I'm learning new things every day. Day is nice.\"\n",
    "tokens = sentence.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Python Concepts \n",
    "\n",
    "As we start working with bigger text documents, sets of tokens and numerical representations, the amount of individual bits of information we need to store and refer back to becomes **massive**. We'll learn some new Python concepts to both handle large collections of data, and to process them.\n",
    "\n",
    "### Lists \n",
    "\n",
    "Previously, we'd used **named variables** to store individual bits of information such as text and numbers:\n",
    "\n",
    "```\n",
    "my_diary_entry = \"Today I mainly SMASHED IT\"\n",
    "hours_spent_smashing_it = 1000\n",
    "```\n",
    "\n",
    "But if we look at the ``split()`` function we have just used, it returns **6 different values**. And it would seem like a lot of effort to have a named variable for each of them?\n",
    "\n",
    "```\n",
    "token_1 = \"I'm\"\n",
    "token_2 = \"learning\"\n",
    "token_3 = \"new\"\n",
    "token_4 = \"things\"\n",
    "token_5 = \"every\"\n",
    "token_6 = \"day\"\n",
    "```\n",
    "\n",
    "And what happens when we have 1000 tokens? Or 30,000 (this is the size of the average english speakers vocabulary)?\n",
    "\n",
    "Instead, we can store collections of values in a **single object** known as a ``List``. You may also here the terms ``array`` and ``vector``, and whilst they do mean specific things in specific circumstances, these are all broadly interchangeable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tokens is a variable that contains a list\n",
    "print(tokens)\n",
    "print(\"the third item is ->\", tokens[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Lists\n",
    "\n",
    "We can access items in a List by using ``numerical indexes`` in `square brackets []`. \n",
    "\n",
    "At this point, you might start to find things familiar from Week 1 when we looked at Strings. In fact, **Strings can kind of be considered as Lists of Characters**. \n",
    "\n",
    "Know I've blown your minds, one things to be wary of:\n",
    "\n",
    "#### In computer science, we start counting at 0\n",
    "\n",
    "That means the first item in a list is\n",
    "\n",
    "``my_list[0]``\n",
    "\n",
    "And the second item in the array is \n",
    "\n",
    "``my_list[1]``\n",
    "\n",
    "If you give an index that is longer than the list, **you will get an error!**\n",
    "\n",
    "Like any other variable, **you can also overwrite** items in a list \n",
    "\n",
    "``\n",
    "my_list[0] = 1\n",
    "``\n",
    "\n",
    "``\n",
    "camera_locations[3] = \"hilltop\"\n",
    "``\n",
    "\n",
    "### Adding new values \n",
    "\n",
    "We can also **extend** an existing list using the `append()` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)\n",
    "\n",
    "tokens.append(\"!\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or **overwrite** a list values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \"hi\" # this is a string\n",
    "print(tokens)\n",
    "tokens = \"I keep on learning things, its almost like I went back to school\".split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Dimensional ``Lists``\n",
    "\n",
    "The ``Lists`` we have seen up until this point have mainly been 1-dimensional, that is, all the items in the list are just single objects like numbers or strings. But, it is possible to have lists in multiple dimensions and for the time being we will just move to 2. \n",
    "\n",
    "It can help to think of a 1D ``List`` as a queue, or a shopping list. There are only two directions (backwards and forwards) and you only need **1 index** to access an item in it. \n",
    "\n",
    "You can think of 2D ``List`` is a grid, so more like a chess board. You can move in 4 directions (forwads, backwards, left and right) and you need **2 indexes** to access any item.\n",
    "\n",
    "Technically, in a 2D ``List``, each item of the outer array is also another 1D ``List``.\n",
    "\n",
    "Taking from mathematics, these 1D ``Lists`` are often called **vectors** and 2D ``Lists`` are called **matrices** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = [[1,2],[3,4]]\n",
    "\n",
    "#m is a matrix\n",
    "print(\"a matrix:\",m)\n",
    "\n",
    "#Get the first row (a vector)\n",
    "print(\"a vector (a row of the matrix, in this case, the first row):\", m[0])\n",
    "\n",
    "#Get a specific item [row, col]\n",
    "print(\"a scalar (an item in the matrix, in this case, in the first row, second column):\",m[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Loops\n",
    "\n",
    "**For loops** are used when we want to do a repeated action for a given number of times. The code below shows the standard structure:\n",
    "\n",
    "- The first line (`for i in array:`) tells us to take every item in the List `array` **in order**, and store it in `i`. \n",
    "\n",
    "- The code underneath dictates what repeated actions we do with i each time and **must** be indented with a tab, otherwise Python will complain. The actions to be repeated can be a single line code, or multiple lines. Every line that is indented will be included in the loop and executed each time.\n",
    "\n",
    "Note: these two blocks of code below are **pseudocode**. Pseudocode is used to communicate (to a human) how an algorithm works, but it is not intended to be executed by a machine. For the first block below to be executed, we would need to declare the array (e.g., `array = [1,2,3]`) and the function `do_something_with_i` (e.g., `def do_something_with_i(): print(i)`).\n",
    "\n",
    "```\n",
    "for i in array:\n",
    "    do_something_with_i \n",
    "#end of loop\n",
    "```\n",
    "\n",
    "```\n",
    "for i in array:\n",
    "     do_something_with_i\n",
    "     do_something_else_with_i\n",
    "     do_another_thing\n",
    "#end of loop\n",
    "```  \n",
    "\n",
    "Below are some examples of loops (written in actual python and not pseudocode!):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tokens: \",tokens)\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For loop prints out every item in turn\n",
    "a = [[1,2],[2,3]]\n",
    "\n",
    "for number_list in a:\n",
    "    for number in number_list:\n",
    "        print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = range(len(tokens))\n",
    "\n",
    "range_tokens = [0,1,2,3,4,5,6,7,8,9,10,11,12,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_tokens = [0,1,0,0,4,4,6,7,8,9,10,11,12]\n",
    "#Use range to get a sequence of numbers from 0->length of array\n",
    "indexes = range(len(tokens))\n",
    "\n",
    "for i in range_tokens: # also commonly written as `for i in range(len(tokens))``\n",
    "    print(i,\":\",tokens[i])\n",
    "\n",
    "print(\"end of for loop 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can have multiple lines of code in the loop\n",
    "#They are all indented \n",
    "for t in tokens:\n",
    "    print(t)\n",
    "    print(len(t)) # t is a string, so len(t) gives the number of characters\n",
    "    print(\"this happens every time\")\n",
    "print(\"end of for loop 3, this only happens once\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you are not sure what type of variable you are dealing with, you can do `type(the_variable)`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"type of tokens: \",type(tokens))\n",
    "a=\"hi\" \n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to NLP\n",
    "\n",
    "You probably won't understand all of the code in the following parts, but thats fine. You're Great. What's important is that you get the general idea of what we're doing. The coding specifics will come with practise!\n",
    "\n",
    "We've picked up some super important new concepts that we'll keep practising throughout the course. We'll start with some new concepts for analysing text:\n",
    "\n",
    "### How good is our vocabulary?\n",
    "\n",
    "We have made a vocabulary from, (or **tokenised**, if you're fancy) our text document by splitting every time we see a space.\n",
    "\n",
    "Does this seem sensible? Does this capture every thing that we would consider a separate word in the document? \n",
    "\n",
    "What about `isn't`? Is this one token (`isn't`), or two tokens (`is` and `not`)? Or `taxi cab`? Is that two tokens, do we care that `taxi` and `cab` are both used? Do we need this as a separate concept from `Uber`, `limo`, `Hackney Carriage` or `car`?. If we take it as one, do we miss out on other combinations like `black cab` or `taxi driver`? What about punctuation?\n",
    "\n",
    "Just using `str.split()` works reasonably well on the sentence below. However, there are some issue with punctuation. Ideally, the brackets and the exclamation mark would be separate tokens. We will deal with this later.\n",
    "\n",
    "After we have split our sentence into tokens, we can then create a **vocabulary**, which contains every unique token in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#String split\n",
    "sentence = \"I like to think (it has to be!) of a cybernetic ecology where we are free of our labors\"\n",
    "tokenised = sentence.split()\n",
    "print(\"Tokenised sentence: \")\n",
    "print(tokenised)\n",
    "print(len(tokenised))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the unique tokens (removes duplicates)\n",
    "vocab = np.unique(tokenised)\n",
    "print(\"\\n Vocabulary (unique tokens):\") # \\n adds a line break\n",
    "print(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "Now that we have derived a **vocabulary** (if not exactly perfect yet) for the sentence, we can represent it as a set of numbers. \n",
    "\n",
    "To create a **one-hot encoding**, we assign **each token in a document** a vector that is the length of the vocabulary, with each slot in this vector representing a token in the vocabulary. These slots can either be **1 or 0**.\n",
    "\n",
    "For the slot that represents that token in the vocabulary, we set to 1. Every other slot is 0. \n",
    "\n",
    "This leaves us with a **2-d List**, where each row is a list as long as the vocabulary. Each row only ever has a single 1 in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into tokens based on spaces\n",
    "tokenised = sentence.split()\n",
    "print('tokenised: ', tokenised, len(tokenised))\n",
    "\n",
    "#Get the unique tokens\n",
    "vocab = np.unique(tokenised)\n",
    "print('vocab: ', vocab, len(vocab))\n",
    "\n",
    "\n",
    "#Make a matrix of zeros using the lengths of the separated sentence and vocab\n",
    "one_hot = np.zeros((len(tokenised), len(vocab)))\n",
    "\n",
    "\n",
    "#Go through the separated sentence and set the appropriate item to 1\n",
    "for i in range(len(tokenised)):\n",
    "    #Get the word\n",
    "    word = tokenised[i]\n",
    "    #find the index of the word in the vocab\n",
    "    match = np.where(vocab == word)\n",
    "    #Set it to 1 (hot)\n",
    "    one_hot[i, match] = 1\n",
    "    \n",
    "print(pd.DataFrame(one_hot, columns = vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **one-hot encoding** doesn't lose any information. We keep a reference to every token, as well as the sequence in which they appear. As we have seen, small differences and nuance in natural language can have big effects in meaning.\n",
    "\n",
    "**But its a lot of numbers** for a small amount of information. This being said, it is also super **sparse**, which just means there are lots of 0s, and there are actually lots of techinques for really efficiently storing sparse data. \n",
    "\n",
    "We've successfully represented our sentence as a maxtrix of numbers, which we can use with various mathematical techniques moving forwards. \n",
    "\n",
    "### Bag of Words\n",
    "\n",
    "Using **one-hot encoding**, we have a **vector** the length of the vocabulary for every word. However, this can quickly get out of hand with longer documents and bigger vocabularies.\n",
    "\n",
    "One improvement we can make to this representation is to simply count up every occurrence of each word in the vocabulary for each document, and then store this count for each word in the vocabulary. This is what we call a bag of words, in which we represent a document by its words and the frequency at which they appear.\n",
    "\n",
    "This means we only have one **word frequency vector** for each document, rather than a one-hot encoding vector for each word. If we had multiple documents (or sentences), we could make a **word frequency vector** for each one and store them as a **matrix** (2D array).\n",
    "\n",
    "The dictionary that we get out of the Counter object is an efficient storage method, as absent words are just ignored.\n",
    "\n",
    "Even though this a big compression of the data, this approach actually ends up not losing much of the meaning of the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a Counter to create a Bag of Words (word-frequency vector)\n",
    "from collections import Counter\n",
    "bow = Counter(tokenised)\n",
    "bow # in a jupyter notebook, you can print an array by just writing its name if it's at the end of the cell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Books\n",
    "Here we have a novel from [Project Gutenberg](https://gutenberg.org/ebooks/). Its about hacking is and copyright free. Lets see what we can find out about it.\n",
    "\n",
    "What can we find out about each chapter by counting the amount of times a word appears in each? Are any similar to each other? How can we adjust our vocabulary to help us out? First we're going to look at the book as a whole.\n",
    "\n",
    "We're going to use a number of techniques to try and tweak our vocabulary so that it contains the most information for when we start using this bag of words in tasks like topic modelling or classification. \n",
    "\n",
    "This means we want to count things that have the same meaning as the same token, but we also don't want to throw away any information that might help our processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = open('../data/hacking.txt', 'r') \n",
    "book = fs.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokens is a 1D array\n",
    "tokens = book.split() \n",
    "#Get the unique tokens (our vocabulary)\n",
    "vocab = np.unique(tokens)\n",
    "print(\"total words:\",len(tokens), \"unique words:\", len(vocab))\n",
    "#Create a Bag of Words using a Counter\n",
    "Counter(tokens).most_common(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the split function does splits the document by spaces and does not consider punctuation, punctuation is included as part of the word, so `park` and `park.` or `park?` are added to the vocabulary as different tokens. We can use a regular expression to identify those duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicate words that might be taken \n",
    "import re \n",
    "ctr = 0; # counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocab: \n",
    "    if len(re.findall(r'[\\.,;!?]$', word)) > 0: # `re.findall(r'[\\.,;!?]$', word)` returns an empty list if the word does not end ($) in . , ; ! or ?.\n",
    "        if word[:-1] in vocab:\n",
    "            ctr = ctr + 1 # this is also commonly written as `ctr +=1`\n",
    "print(ctr,\" duplicate words ending in [.,;!?] out of \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"hello!\"\n",
    "print(word[-1])\n",
    "print(word[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuation duplicates\n",
    "We can see lots of tokens have trailing punctutation, and a good few of these will also exist in without the punctuation. This duplication is bad for us!\n",
    "\n",
    "We would want these to be the same token so we can use a **regex** to replace it. The regex below splits on whitespace (represented by `\\s`), hyphen (`-`) or punctuation (`.,;!?()`) that appears at least once (using this plus notation `+`). We immediately see the size of the vocabulary drops by about 25%, showing there was loads of duplication in our bag of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use a regex to split based on space AND punctuation\n",
    "tokens = re.split(r'[-\\s.,;!?()]+', book) \n",
    "vocab = np.unique(tokens)\n",
    "print(\"unique words\", vocab.shape)\n",
    "Counter(tokens).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now clean the initial example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#String split\n",
    "sentence = \"I like to think (it has to be!) it's of a cybernetic ecology where we are free of our labors\"\n",
    "_tokenised = sentence.split()\n",
    "print(\"Tokenised sentence\")\n",
    "print(_tokenised)\n",
    "\n",
    "#Use a regex to split based on space AND punctuation\n",
    "_tokens = re.split(r'[-\\s.,;!?()]+', sentence)\n",
    "vocab = np.unique(_tokens)\n",
    "print(\"unique words\", vocab.shape)\n",
    "print(vocab)\n",
    "#Counter(_tokens).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "We can see that the commonly occurring words don't tell us much about this specific book. Traditionally, in NLP it has been useful to remove words that occur commonly. They don't tell us very much about each document because they are contained in almost all the documents. These are known as **stop words**. Examples: the, a, she, he, his, her, to, was...\n",
    "\n",
    "In contemporary NLP we often don't actually remove stop words because we have the computing power to deal with the extra vocab size and any information we throw away can effect performance, especially in deep learning, and especially when we start to look at context of sequences of words. \n",
    "\n",
    "Here we see a list from the sklearn library (each library has its own list of stop words). \n",
    "\n",
    "We'll just see how removing stop words from our bag effects what we can see. Although our vocabulary size is basically the same (so we're not saving much in effiency), the list most common words are much more informative and tells us more about the specific book we're looking at. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install library if not already installed\n",
    "%pip install scikit-learn # in a jupyter notebook, the % command sends the commands after it to the terminal  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import _stop_words # stop words are in lower case\n",
    "tokens_without_stop_words = []\n",
    "for t in tokens:\n",
    "    if not t in _stop_words.ENGLISH_STOP_WORDS: \n",
    "        tokens_without_stop_words.append(t)\n",
    "stop_vocab = np.unique(tokens_without_stop_words)\n",
    "print(\"unique words\", len(stop_vocab))\n",
    "Counter(tokens_without_stop_words).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming attempt to remove suffixes from words that contain the same base. This reduces variation and can help when we reduce the documents into a more distilled form (like a bag of words). \n",
    "\n",
    "- hacking, hackers, hacked, hacks\n",
    "- computer, computing, computers\n",
    "\n",
    "Depending on our task, it might be the case that we only really care about knowing if **any** of these words appear, not whether they each appear individually. For example, I might be doing a search for paragraphs about hacking and it may be that I would miss out on key documents otherwise if I only searched for one of the words. \n",
    "\n",
    "Stemming can be quite a challenging task however. If we want to combine pluralisations, for example, we can't just remove the \"s\" from the end of all nouns, what about \n",
    "\n",
    "- grass (not a plural)\n",
    "- mice, octopi (plural, no s)\n",
    "- geniuses (plural, es)\n",
    "\n",
    "We're going to use the built-in stemmer in the NLTK library. This reduces our vocabulary in the hacking book dramatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the nltk library \n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "word_list = ['feet', 'foot', 'foots', 'footing']\n",
    "for word in word_list:\n",
    "    print(word, \"->\", stemmer.stem(word))\n",
    "#Doesn't always work\n",
    "word_list = ['organise','organises','organised','organisation',\"organs\",\"organ\",\"organic\"]\n",
    "for word in word_list:\n",
    "    print(word, \"->\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Looking at our hacking book\n",
    "stem_tokens = []\n",
    "for t in tokens_without_stop_words:\n",
    "    stem_tokens.append(stemmer.stem(t))\n",
    "stem_vocab = np.unique(stem_tokens)\n",
    "print(\"unique words\", stem_vocab.shape)\n",
    "Counter(stem_tokens).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation \n",
    "Lemmatisation is a technique similar to stemming, apart from it attempts to find similar meanings, as opposed to just similar roots. Like with all these _normalisation_ techniques, reducing your vocabulary will reduce precision but may make your model better at generalising and more efficient.\n",
    "\n",
    "For example, lemmatisation would be able to separate **dogged** and **dog**, which have quite different meanings but would get combined by a stemmer. \n",
    "\n",
    "Below we use the WordNetLemmatizer from the NLTK library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"lemmatize \\\"dogged\\\": \", lem.lemmatize(\"dogged\", pos=\"a\"))\n",
    "print(\"lemmatize \\\"dog\\\": \", lem.lemmatize(\"dog\", pos=\"n\"))\n",
    "print(\"stem \\\"dogged\\\": \", stemmer.stem(\"dogged\"))\n",
    "print(\"stem \\\"dog\\\": \", stemmer.stem(\"dog\"))\n",
    "print('\\n')\n",
    "\n",
    "# the `pos` (part-of-speech/grammatical function) tag\n",
    "print(lem.lemmatize(\"better\")) # pos is by default 'n' --> it will try to find the closest noun which might not be ideal\n",
    "print(lem.lemmatize(\"better\", pos =\"v\")) # let's try to find the closest verb --> fails too\n",
    "print(lem.lemmatize(\"better\", pos =\"a\")) # returns 'good' which is a suitable adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Looking at our hacking book\n",
    "lem_tokens = []\n",
    "for t in tokens_without_stop_words:\n",
    "    lem_tokens.append(lem.lemmatize(t))\n",
    "lem_vocab = np.unique(lem_tokens)\n",
    "print(\"unique words\", lem_vocab.shape)\n",
    "Counter(lem_tokens).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capitalisation\n",
    "Whilst it may be tempting to just lower case every token with the belief that words all have the same meaning regardless of case. However, it may actually be that if something is in ALL CAPS it conveys some meaning. Or if a word is at the start of sentence, that has importance. \n",
    "\n",
    "For example \n",
    "\n",
    "- John liked to help\n",
    "- John screamed HELP HELP HELP\n",
    "\n",
    "Or if one book contained lots of capitalised nouns (like cities and countries), it might tell you it was about Geography.\n",
    "\n",
    "Some libraries actually account for this by lower casing everything, then having a token which indicates a start of capitilising as well as one that signifies the end of capitalising. This allows the best of both worlds. Of course, this only works if your model or vocabulary is able to take sequence and context into account. Like for example...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lower_tokens = []\n",
    "for word in lem_tokens:\n",
    "    lower_tokens.append(word.lower())\n",
    "lower_vocab = np.unique(lower_tokens)\n",
    "print(\"unique words\", lower_vocab.shape)\n",
    "bow = Counter(lower_tokens)\n",
    "bow.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-Grams\n",
    "When we do Bag of Words, what we gain in effiency and generalisation, we lose in context. We can gain lots by including combinations of words as tokens, and these are known as **n-grams**. For example, if our book was a tale about an butcher / computer programmer / hacky-sack apologist, we could could see the term `hacking` in a bunch of contexts. If we don't look at the subsequent token, we lose this information.\n",
    "\n",
    " - hacking meat\n",
    " - hacking computers\n",
    " - hacking sack\n",
    " \n",
    "On a more realistic note, I see that `phone` and `network` appear almost exactly the same amount of times, and I'd wager this is because they follow one another and using multiword tokens will capture this connection. Also consider **not**. A newspaper article that said someone was `not guilty` and `not under arrest` and `not a bad chap`, could be summarised with `guilty`, `arrest` and `bad` with just single word tokens.\n",
    " \n",
    "A 2-gram makes a token out of every pair of subsequent words, and a 3-gram out of every 3 etc.\n",
    "\n",
    "What we see when we apply this to our book is that there is a massive explosion in the number of tokens. Which makes sense, as we're counting sequences and its likely that words will appear in **more than one sequence**, and that **exact matching sequences will appear less times that single words**. \n",
    "\n",
    "But we also see some new things, for example, `australian` by itself didn't make the top 50, but when combined with to make `australian hacker`, it becomes much more common in respect to the whole document. This quickly lets us know perhaps of all the hackers in the book, an australian one is the most prominent. Or if there's only one hacker, they're most prominent feature is their australian-ness. \n",
    "\n",
    "And although '`computer` is very regular in the single word list, we get more context with 2-grams such as `computer undergroud` and `computer crime`.\n",
    "\n",
    "Some of these combinations will be super rare and its common to drop any n-grams that don't appear often from your vocabulary. When your feature vector (the size of the bag of words) gets bigger than the number of documents you have, you start to get problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "ngram_tokens = []\n",
    "#Loop through all the tokens\n",
    "for i in range(len(lower_tokens)-n):\n",
    "    word = lower_tokens[i]\n",
    "    #Stich together with subsequent tokens into one string\n",
    "    for j in range(n - 1):\n",
    "        word = word + \" \" + lower_tokens[i + 1 + j]\n",
    "    #Add to list\n",
    "    ngram_tokens.append(word)\n",
    "ngram_vocab = np.unique(ngram_tokens)\n",
    "print(\"unique n-grams\", ngram_vocab.shape)\n",
    "Counter(ngram_tokens).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ab31ad0604042712fead20041f419ee31fb8ef134e5de51d0e27c30f5676e5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
