{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Classification with topic modelling features (using BoW and LDA).\n",
    "\n",
    "In this notebook we are going to perform classification on a text dataset using features from topic modelling. There is a similiar notebook to this that just uses Bag of Words as the text feature for classification. \n",
    "\n",
    "This notebook uses the [Myers-Briggs comments dataset](https://git.arts.ac.uk/tbroad/myers-briggs-comments-dataset) that is scraped from the website [16personalities.com](https://www.16personalities.com/) using [this web-crawling code](https://git.arts.ac.uk/tbroad/web-scrape-myers-briggs). The [Myers-Briggs personality typology](https://en.wikipedia.org/wiki/Myers%E2%80%93Briggs_Type_Indicator) is a categorisation for personality types that result from a questionnaire that asks people how they make decision and percieve the world. The questionnaire breaks people down into 16 personality types, made up of four binary categories:\n",
    "\n",
    "![Myers-Briggs personality types](../media/MyersBriggsTypes.png)\n",
    "\n",
    "The website 16 personalities offers these questionairres, while also giving information about each personality type. On this website people can make accounts, and when they have done the test, their personality type appears on their user profile. Each page has a comments section and this dataset is made from exhuastively scraping all of these comments, along with their associated personality type. In addition to the 16 personalities, the website goes further to add the `-T` and `-A` types (being 'Turbulent' and 'Assertive'), meaning they actual end up tracking 32 personality types (2 to the power 5). These personality types is something we will learn about in more detail in Personalisation and Machine Learning in Term 3.\n",
    "\n",
    "We are going to use this dataset to do some classification, looking at the many different ways we can split this dataset to see which factors of personality are most easily predicted based on. Then we will look at different ways we might try and improve our accuracy with classification.\n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK utils\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Bag of words and LDA \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Classification stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And download these if we haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ROG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ROG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ROG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util function for part of speech tagging for lemmatisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function originally from: https://www.programcreek.com/python/?CodeExample=get%20wordnet%20pos\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets load and look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>personality type</th>\n",
       "      <th>source url</th>\n",
       "      <th>comment</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>parent_comment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77016</td>\n",
       "      <td>INFP-A</td>\n",
       "      <td>https://www.16personalities.com/infp-strengths...</td>\n",
       "      <td>Hello friends infp, I identify a lot with all ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77025</td>\n",
       "      <td>ISTP-T</td>\n",
       "      <td>https://www.16personalities.com/infp-strengths...</td>\n",
       "      <td>I can't believe how accurate this was, it's so...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77700</td>\n",
       "      <td>INFP-T</td>\n",
       "      <td>https://www.16personalities.com/infp-strengths...</td>\n",
       "      <td>We matter. I am Grace too.  It is so refreshin...</td>\n",
       "      <td>True</td>\n",
       "      <td>77073.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77073</td>\n",
       "      <td>INFP-T</td>\n",
       "      <td>https://www.16personalities.com/infp-strengths...</td>\n",
       "      <td>Finally I know for sure that I am not a weirdo...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77151</td>\n",
       "      <td>INFP-T</td>\n",
       "      <td>https://www.16personalities.com/infp-strengths...</td>\n",
       "      <td>I finally feel understood. I always give and g...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41695</th>\n",
       "      <td>119246</td>\n",
       "      <td>ENTP-T</td>\n",
       "      <td>https://www.16personalities.com/entp-personality</td>\n",
       "      <td>I'm such a debater I had to debate before deci...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41696</th>\n",
       "      <td>119900</td>\n",
       "      <td>ENTP-A</td>\n",
       "      <td>https://www.16personalities.com/entp-personality</td>\n",
       "      <td>Accurate</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697</th>\n",
       "      <td>121460</td>\n",
       "      <td>ENTP-A</td>\n",
       "      <td>https://www.16personalities.com/entp-personality</td>\n",
       "      <td>Debatable</td>\n",
       "      <td>True</td>\n",
       "      <td>119995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41698</th>\n",
       "      <td>120098</td>\n",
       "      <td>ENTP-T</td>\n",
       "      <td>https://www.16personalities.com/entp-personality</td>\n",
       "      <td>relatable</td>\n",
       "      <td>True</td>\n",
       "      <td>119995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41699</th>\n",
       "      <td>119995</td>\n",
       "      <td>ENTP-T</td>\n",
       "      <td>https://www.16personalities.com/entp-personality</td>\n",
       "      <td>I debated the actuality of being being a debat...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41700 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       comment_id personality type  \\\n",
       "0           77016           INFP-A   \n",
       "1           77025           ISTP-T   \n",
       "2           77700           INFP-T   \n",
       "3           77073           INFP-T   \n",
       "4           77151           INFP-T   \n",
       "...           ...              ...   \n",
       "41695      119246           ENTP-T   \n",
       "41696      119900           ENTP-A   \n",
       "41697      121460           ENTP-A   \n",
       "41698      120098           ENTP-T   \n",
       "41699      119995           ENTP-T   \n",
       "\n",
       "                                              source url  \\\n",
       "0      https://www.16personalities.com/infp-strengths...   \n",
       "1      https://www.16personalities.com/infp-strengths...   \n",
       "2      https://www.16personalities.com/infp-strengths...   \n",
       "3      https://www.16personalities.com/infp-strengths...   \n",
       "4      https://www.16personalities.com/infp-strengths...   \n",
       "...                                                  ...   \n",
       "41695   https://www.16personalities.com/entp-personality   \n",
       "41696   https://www.16personalities.com/entp-personality   \n",
       "41697   https://www.16personalities.com/entp-personality   \n",
       "41698   https://www.16personalities.com/entp-personality   \n",
       "41699   https://www.16personalities.com/entp-personality   \n",
       "\n",
       "                                                 comment  is_reply  \\\n",
       "0      Hello friends infp, I identify a lot with all ...     False   \n",
       "1      I can't believe how accurate this was, it's so...     False   \n",
       "2      We matter. I am Grace too.  It is so refreshin...      True   \n",
       "3      Finally I know for sure that I am not a weirdo...     False   \n",
       "4      I finally feel understood. I always give and g...     False   \n",
       "...                                                  ...       ...   \n",
       "41695  I'm such a debater I had to debate before deci...     False   \n",
       "41696                                           Accurate     False   \n",
       "41697                                          Debatable      True   \n",
       "41698                                          relatable      True   \n",
       "41699  I debated the actuality of being being a debat...     False   \n",
       "\n",
       "       parent_comment_id  \n",
       "0                    NaN  \n",
       "1                    NaN  \n",
       "2                77073.0  \n",
       "3                    NaN  \n",
       "4                    NaN  \n",
       "...                  ...  \n",
       "41695                NaN  \n",
       "41696                NaN  \n",
       "41697           119995.0  \n",
       "41698           119995.0  \n",
       "41699                NaN  \n",
       "\n",
       "[41700 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/myers_briggs_comments.tsv', sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can delete the columns `comment_id` and `parent_comment_id` (we aren't going to use the columns `source url` and `is_reply`, but they may come in handy in the bonus tasks later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personality type</th>\n",
       "      <th>source url</th>\n",
       "      <th>comment</th>\n",
       "      <th>is_reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFP-A</td>\n",
       "      <td>https://www.16personalities.com/infp-strengths...</td>\n",
       "      <td>Hello friends infp, I identify a lot with all ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISTP-T</td>\n",
       "      <td>https://www.16personalities.com/infp-strengths...</td>\n",
       "      <td>I can't believe how accurate this was, it's so...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFP-T</td>\n",
       "      <td>https://www.16personalities.com/infp-strengths...</td>\n",
       "      <td>We matter. I am Grace too.  It is so refreshin...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFP-T</td>\n",
       "      <td>https://www.16personalities.com/infp-strengths...</td>\n",
       "      <td>Finally I know for sure that I am not a weirdo...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFP-T</td>\n",
       "      <td>https://www.16personalities.com/infp-strengths...</td>\n",
       "      <td>I finally feel understood. I always give and g...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41695</th>\n",
       "      <td>ENTP-T</td>\n",
       "      <td>https://www.16personalities.com/entp-personality</td>\n",
       "      <td>I'm such a debater I had to debate before deci...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41696</th>\n",
       "      <td>ENTP-A</td>\n",
       "      <td>https://www.16personalities.com/entp-personality</td>\n",
       "      <td>Accurate</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697</th>\n",
       "      <td>ENTP-A</td>\n",
       "      <td>https://www.16personalities.com/entp-personality</td>\n",
       "      <td>Debatable</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41698</th>\n",
       "      <td>ENTP-T</td>\n",
       "      <td>https://www.16personalities.com/entp-personality</td>\n",
       "      <td>relatable</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41699</th>\n",
       "      <td>ENTP-T</td>\n",
       "      <td>https://www.16personalities.com/entp-personality</td>\n",
       "      <td>I debated the actuality of being being a debat...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41700 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      personality type                                         source url  \\\n",
       "0               INFP-A  https://www.16personalities.com/infp-strengths...   \n",
       "1               ISTP-T  https://www.16personalities.com/infp-strengths...   \n",
       "2               INFP-T  https://www.16personalities.com/infp-strengths...   \n",
       "3               INFP-T  https://www.16personalities.com/infp-strengths...   \n",
       "4               INFP-T  https://www.16personalities.com/infp-strengths...   \n",
       "...                ...                                                ...   \n",
       "41695           ENTP-T   https://www.16personalities.com/entp-personality   \n",
       "41696           ENTP-A   https://www.16personalities.com/entp-personality   \n",
       "41697           ENTP-A   https://www.16personalities.com/entp-personality   \n",
       "41698           ENTP-T   https://www.16personalities.com/entp-personality   \n",
       "41699           ENTP-T   https://www.16personalities.com/entp-personality   \n",
       "\n",
       "                                                 comment  is_reply  \n",
       "0      Hello friends infp, I identify a lot with all ...     False  \n",
       "1      I can't believe how accurate this was, it's so...     False  \n",
       "2      We matter. I am Grace too.  It is so refreshin...      True  \n",
       "3      Finally I know for sure that I am not a weirdo...     False  \n",
       "4      I finally feel understood. I always give and g...     False  \n",
       "...                                                  ...       ...  \n",
       "41695  I'm such a debater I had to debate before deci...     False  \n",
       "41696                                           Accurate     False  \n",
       "41697                                          Debatable      True  \n",
       "41698                                          relatable      True  \n",
       "41699  I debated the actuality of being being a debat...     False  \n",
       "\n",
       "[41700 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('comment_id', axis=1)\n",
    "df = df.drop('parent_comment_id', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizer\n",
    "\n",
    "Now lets run our lemmatizer on the comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\genericpath.py:42\u001b[0m, in \u001b[0;36misdir\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(s)\n\u001b[0;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 系统找不到指定的文件。: 'c:\\\\Users\\\\ROG\\\\anaconda3\\\\envs\\\\stem\\\\nltk_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ROG\\Documents\\GitHub\\NLP_TianQiu\\Week-6-Classification\\mb-classification-with-lda-features.ipynb 单元格 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     comment \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(row[\u001b[39m'\u001b[39m\u001b[39mcomment\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     lemmitized_comment \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([lemmatizer\u001b[39m.\u001b[39mlemmatize(word, get_wordnet_pos(word)) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m comment\u001b[39m.\u001b[39msplit()])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     df\u001b[39m.\u001b[39mloc[index, \u001b[39m'\u001b[39m\u001b[39mcomment\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m lemmitized_comment\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df\n",
      "\u001b[1;32mc:\\Users\\ROG\\Documents\\GitHub\\NLP_TianQiu\\Week-6-Classification\\mb-classification-with-lda-features.ipynb 单元格 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     comment \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(row[\u001b[39m'\u001b[39m\u001b[39mcomment\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     lemmitized_comment \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([lemmatizer\u001b[39m.\u001b[39mlemmatize(word, get_wordnet_pos(word)) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m comment\u001b[39m.\u001b[39msplit()])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     df\u001b[39m.\u001b[39mloc[index, \u001b[39m'\u001b[39m\u001b[39mcomment\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m lemmitized_comment\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df\n",
      "\u001b[1;32mc:\\Users\\ROG\\Documents\\GitHub\\NLP_TianQiu\\Week-6-Classification\\mb-classification-with-lda-features.ipynb 单元格 12\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_wordnet_pos\u001b[39m(word):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     tag \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mpos_tag([word])[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mupper()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     tag_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mJ\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39mADJ,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mN\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39mNOUN,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mV\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39mVERB,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mR\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39mADV}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-6-Classification/mb-classification-with-lda-features.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tag_dict\u001b[39m.\u001b[39mget(tag, wordnet\u001b[39m.\u001b[39mNOUN)\n",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos_tag\u001b[39m(tokens, tagset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[39m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[39m=\u001b[39m PerceptronTagger()\n\u001b[0;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         find(\u001b[39m\"\u001b[39;49m\u001b[39mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m PICKLE)\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\site-packages\\nltk\\data.py:530\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[39m# Is the path item a directory or is resource_name an absolute path?\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m path_ \u001b[39mor\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49misdir(path_):\n\u001b[0;32m    531\u001b[0m     \u001b[39mif\u001b[39;00m zipfile \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m         p \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path_, url2pathname(resource_name))\n",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\genericpath.py:42\u001b[0m, in \u001b[0;36misdir\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return true if the pathname refers to an existing directory.\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(s)\n\u001b[0;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for index, row in df.iterrows():\n",
    "    comment = str(row['comment'])\n",
    "    lemmitized_comment = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in comment.split()])\n",
    "    df.loc[index, 'comment'] = lemmitized_comment\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract classification categories\n",
    "\n",
    "\n",
    "We aren't going to try and classify all 32 personality types (for now), we are just going to look at the first category (Extraversion vs Introversion). As our personality type data is structured using these handy codes, all we need to do is extract the first character from the string to do this (using `df['personality type'].str.strip().str[0]`). We will come back to this code later to try other ways of dividing our dataset.\n",
    "\n",
    "The class `LabelEncoder` is a handy tool to then convert whatever classes we have into integer numbers starting from one (that we need to have for our classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['feat_to_classify'] = df['personality type'].str.strip().str[0]\n",
    "df['class_label'] = le.fit_transform(df['feat_to_classify'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets extract our comments, class labels and the associated names of our classes into Python lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df[\"comment\"].values.tolist()\n",
    "class_labels = df[\"class_label\"].values.tolist()\n",
    "class_names = list(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words features\n",
    "\n",
    "Lets fit our bag of words to our entire dataset first so that the our bag of words feature vectors are the same length in both our test and train sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'), ngram_range=(1,1))\n",
    "bag_of_words = vectorizer.fit_transform(comments)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(f'Our bag of words for the whole dataset is a matrix of the shape and size {bag_of_words.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split dataset\n",
    "\n",
    "Now lets split our dataset into **train** and **test** sets. The training set will be used to optimise our classifier on the data. The test set is used to evaluate our classifier after training. \n",
    "\n",
    "Here `X_train` and `X_test` are our comments. `y_train` and `y_test` are our class labels corresponding to each comment. Our classify will take the bag of words representations of our comments data as input and try to give the most accurate predictions of classes. \n",
    "\n",
    "It is very important that we **never evaluate a classifer on our training data**, and that **we never train on our test data**. When we do training we repeatedly optimise on that data. Therefore the accuracy in training won't give us an accurate idea of how well our classifer is performing. We can only determine a realsitic idea of accuracy on **unseen data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(comments, class_labels, test_size=0.3, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets redo the bag of words on the test and train with .transform() instead of .fit_transform() to ensure we use the complete vocabulary for both the test and train sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = vectorizer.transform(X_train)\n",
    "X_train_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate topics\n",
    "\n",
    "In this cell we will set the number of topics that we want to use for topic modelling, define our topic modelling algorithm and fit the topic modelling algorithm to our training data.\n",
    "\n",
    "*Note that we do **not** want to fit the topic modeling algorithm to the full dataset, which includes training + testing data! This would prevent us from using the testing data to get as accurate an understanding of how well our approach is likely to work on new, **unseen** data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "pd.options.display.max_columns=num_topics #Make sure we display them all\n",
    "labels = ['topic{}'.format(i) for i in range(num_topics)]\n",
    "lda = LatentDirichletAllocation(n_components=num_topics,random_state=123, learning_method='batch')\n",
    "lda_train_topics = lda.fit_transform(X_train_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at our topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_weights = pd.DataFrame(lda.components_.T, index=vocab, columns=labels)\n",
    "topic_weights.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train classifier\n",
    "\n",
    "Now lets define what kind of classifer we are using and train it on our training data. We will give our Bag our words matrix for our entire training set, and out list of classes labels that corresponds to each row in the matrix. The classifier implementation from sci-kit learn will take care of the rest for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(lda_train_topics, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare test data\n",
    "\n",
    "Now lets vectorise our test data and get our associations to our topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bow = vectorizer.transform(X_test)\n",
    "X_test_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_test_topics = lda.transform(X_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test classifier\n",
    "\n",
    "Now we have trained our classifer we can test it. We will get the classifer to make predictions on our test dataset. We will then calucate our accuracy scores by comparing our predictions `y_pred` to our true class labels `y_test`. \n",
    "\n",
    "Sci-kit learn gives us a nice classification report, breaking it down into three scores, **precision**, **recall** and **f1-score**. **Precision** tells us of **True Positives / True Positive + False Positives** (how many retrieved elements are relevant). **Recall** tells us of **True Positives / True Positive + False Negatives** (how many relevant items are retrieved). The **F1-Score** tells us an average (the harmonic mean) of these two scores.\n",
    "\n",
    "<img src=\"../media/precision-recall.png\" alt=\"precision recall diagram\" width=\"500\"/>\n",
    "<img src=\"../media/f1-score.png\" alt=\"F1 score formula\" width=\"500\"/>\n",
    "\n",
    "There is not perfect way to measure accuracy. In some cases, you will be happy with a high recall and low precision if you want to find all possible results, and can use a human expert to check to result (i.e. if you were looking for possible cases of cancer). In other cases you may want high precision but are less bothered about having a high recall (i.e. if were deciding one of many possible stocks to buy that you want to make a profit from).\n",
    "\n",
    "Another analogy would be if you were fishing, recall is **how big your net is** and precision is **how effective your net is at catching fish (and not other things in the sea)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(lda_test_topics)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=class_names)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "**Task 1** Run this notebook and the classification just with Bag of Words features. Which one works better? \n",
    "\n",
    "**Task 2** Try splitting this dataset using some of the other personality distinctions, you can do this by modifying [the cell where we extract the categories we are using for classificaiton](#extract-classification-categories). Try some of the other individual binary distinctions, then see if you can train a classifer on all 16 of the original Myers-Briggs personality types. Can you then do all 32 different categories available to us in the dataset?\n",
    "\n",
    "**Task 3** You may have noticed that we often get perfomance on one of more of the classes we have when we have a large imbalance between the numbers for each class (listed as `support` in our classification report). Try [changing the type of classifier](#train-classifier) used to another one of the [many available classifiers](https://scikit-learn.org/stable/supervised_learning.html) in sci-kit learn.\n",
    "\n",
    "**Task 4** Does [changing the number of topics](#calculate-topics) impact classification accuracy?\n",
    "\n",
    "**Task 5** Can you change the code to use TF-IDF features and LSA instead of BoW and LDA?\n",
    "\n",
    "**Task 6** Discuss with a someone on your table:\n",
    "- What are the potential uses of a text classifier trained on personality characteristics?\n",
    "- What are the ethical concerns of using this dataset?\n",
    "- What are the potential misuses of this dataset? \n",
    "- What are the biases present in this dataset?\n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "**Task A** Can you filter the dataset in some way. For instance you could filter out comments that are replies (using `is_reply` in the dataset) or filter out comments that are below (or above) a certain length. The `source_url` may also be something that you use to filter out particular comments. \n",
    "\n",
    "**Task B** Does using a stemmer instead of a lemmatizer effect the classification scores? What happens if you don't do any pre-processing to the text?\n",
    "\n",
    "**Task C** Can you add any stop words that are specific to this dataset? Does that improve classification results?\n",
    "\n",
    "**Task D** Can you save the results from classification (and any other important meta-data) to a log file. This can just be an append only text file that you log the results of each experiment to, to make comparisons with later. \n",
    "\n",
    "**Task E** If you are doing lots of experiments using the same preprocessing to the text (stemming / lemmatisation), can you perform this and then save that dataset to a separate `.tsv` file. Which then only have to pre-process once, and then can then load directly into your code each time you runa  new experiment?\n",
    "\n",
    "**Task F** Look for other classification datasets on [kaggle](). Can you adapt this notebook to work with a different classification dataset. You may want to make a copy of this notebook before making changes to a new dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
