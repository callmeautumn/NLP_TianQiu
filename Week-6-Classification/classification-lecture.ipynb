{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "import pandas as pd\n",
    "#from nlpia.data.loaders import get_data\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Classification \n",
    "\n",
    "In previous weeks, we've looked at how we can turn text documents into vectors of numbers, and that these numbers give us some idea of the meaning of that text. We can then use these vectors for tasks like search, query and recommendation. \n",
    "\n",
    "### Unsupervised Algorithms\n",
    "\n",
    "When we used techniques like **SVD** or **LDA** to create topics to assign documents to, this was a form of **unsupervised** algorithm. This is because we just gave the text to the algorithm and got it to decide how it wanted to split up the data, and what each topic would represent. \n",
    "\n",
    "The algorithm just invented some topics, then decided how much each document belonged to each topic. We could then go through and manually try to ascribe a theme to each topic if wanted to, for example\n",
    "\n",
    "- Topic 1: The cat topic\n",
    "- Topic 2: The computer topic\n",
    "- Topic 3: The dog food and treehouses topic?\n",
    "\n",
    "Sometimes this worked and there was a clear, human understandable concept that encapsulated the topic well, and sometimes this wasn't possible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Data Frames \n",
    "\n",
    "Hopefully by now we have a good handle on the data structures for holding data in Python. We've seen **vectors** (1D arrays) and **matrices** (2D arrays, or arrays of arrays). These can either be a native Python `list`, or and `numpy array`. \n",
    "\n",
    "We've also seen data frames from the pandas library a few times already, and we've mainly been using them for their nice display qualities, we'll just formally introduce them now as they will start making ever more appearances! \n",
    "\n",
    "You can initialise them from an existing array, and use column and row names to index them instead of just numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising \n",
    "#Pass an array or list into the contructor \n",
    "a = np.arange(9).reshape((3,3))\n",
    "df = pd.DataFrame(a)\n",
    "df = pd.DataFrame(a, columns = [\"col1\",\"col2\",\"another boring name\"])\n",
    "df = pd.DataFrame(a, index = [\"row1\",\"row2\", \"row3 your boat\"], columns = [\"col1\",\"col2\",\"another boring name\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing by name or number\n",
    "print(df[\"col1\"][\"row1\"])\n",
    "print(df[\"col1\"][0])\n",
    "\n",
    "#iloc for indexes\n",
    "print(df.iloc[0])\n",
    "print(df.iloc[1])\n",
    "print(df.iloc[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NewsGroup Dataset\n",
    "\n",
    "This dataset contains newsgroup posts about space and about hockey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/space_hockey.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See some random space posts\n",
    "test = df[\"label\"] == 0\n",
    "random_post = df[test].sample(1)\n",
    "print(random_post[\"text\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See some random hockey posts\n",
    "test = df[\"label\"] == 1\n",
    "random_post = df[test].sample(1)\n",
    "print(random_post[\"text\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I've added in my own tokens I want to remove\n",
    "remove = list(_stop_words.ENGLISH_STOP_WORDS) + [\"%\",\"!\",\",\",\":\",\"@\",\"£\",\"$\",\"&\",\"*\",\"(\",\")\",\"?\",\"<\",\">\",\".\",\"+\",\"_\",\"|\",\"/\",\"-\"]\n",
    "def my_tokeniser(doc):\n",
    "    #\n",
    "    tokens =  casual_tokenize(doc)\n",
    "    processed = []\n",
    "    for t in tokens:\n",
    "        #make lowercase\n",
    "        t = t.lower()\n",
    "        #Remove stop words\n",
    "        if not t in remove:\n",
    "            processed.append(t)\n",
    "    #Return an array of tokens for that document\n",
    "    return processed\n",
    "#Make TFIDF Vectoriser\n",
    "vectoriser = TfidfVectorizer(tokenizer=my_tokeniser)\n",
    "#Fit the model\n",
    "tfidf_model = vectoriser.fit(df[\"text\"])\n",
    "#Get the vocab\n",
    "vocab = np.array(tfidf_model.get_feature_names_out())\n",
    "\n",
    "#Get vectors for everything\n",
    "vectorised = tfidf_model.transform(df[\"text\"]) #.todense()\n",
    "vectorised_df = pd.DataFrame(vectorised.todense(), columns = vocab)\n",
    "#Get vectors for space articles\n",
    "vectorised_space = tfidf_model.transform(df[df[\"label\"] == 0][\"text\"])\n",
    "vectorised_space_df = pd.DataFrame(vectorised_space.todense(), columns = vocab)\n",
    "#Get vectors for hockey articles\n",
    "vectorised_hockey = tfidf_model.transform(df[df[\"label\"] == 1][\"text\"])\n",
    "vectorised_hockey_df = pd.DataFrame(vectorised_hockey.todense(), columns = vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get SVD vectors\n",
    "svd = TruncatedSVD(n_components = 16, n_iter = 100) \n",
    "svd_topic_vectors = svd.fit_transform(vectorised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_weights = pd.DataFrame(svd.components_.T, index=vocab, columns=['topic{}'.format(i) for i in range(16)])\n",
    "topic_weights #display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_weights.topic2.sort_values(ascending=False).head(10) # show top 10 weighted words for topic 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning \n",
    "\n",
    "But what if we had specific labels that we wanted to attach to documents that we knew about before hand? \n",
    "\n",
    "- Is this text spam or not?\n",
    "- Is this book about horror, sci-fi or cooking?\n",
    "- Is this song going to be a hit?\n",
    "\n",
    "This is where **supervised machine learning** comes in, specifically, **classification**. \n",
    "\n",
    "To try and get a feel for the task of classification, we're going to have a look at each class and see if we think it wll be easy to a computer program that can pick apart these two groups.\n",
    "\n",
    "We're going to try an experiment to see if we can find two words that would allow us to decide if a document was from the space group or the hockey group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum up TFIDF score for each word across all documents\n",
    "space_sums = pd.DataFrame(vectorised_space.sum(axis = 0).T, index=vocab, columns = [\"sums\"])\n",
    "hockey_sums = pd.DataFrame(vectorised_hockey.sum(axis = 0).T, index=vocab, columns = [\"sums\"])\n",
    "print(\"\\nSPACE:\\n\", space_sums[\"sums\"].sort_values(ascending=False).head(30), \"\\nHOCKEY:\\n\", hockey_sums[\"sums\"].sort_values(ascending=False).head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the TFxIDF values for a specific token\n",
    "\n",
    "We're going to plot the `TFxIDF` values for **each post** for **two specific tokens**.\n",
    "\n",
    "The arrays that hold our `TFxIDF` values have the shape ``numPost x lengthOfVocab`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorised.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a **filter** to get the indexer of a **given token**\n",
    "\n",
    "``\n",
    "filter = vocab == features[0]\n",
    "``\n",
    "\n",
    "Then we can grab all the rows (using the `colon :`), given that column. \n",
    "\n",
    "We end up with **2 features** for each token, which we can plot on a graph. The colour represents **which news group the post was originally from**.\n",
    "\n",
    "### Indexes **and** objects in `for loops` with `enumerate()`\n",
    "\n",
    "Previously, we have looked at two main approaches to `for loops`. We either iterate through an array and get the **values themselves**\n",
    "\n",
    "```\n",
    "chapters = [\"chapter1\", \"chapter2\", \"chapter3\"]\n",
    "for c in chapters:\n",
    "    analyse(c)\n",
    "\n",
    "```\n",
    "\n",
    "Or we've used `range()` to iterate over some indexes\n",
    "\n",
    "```\n",
    "chapters = [\"chapter1\", \"chapter2\", \"chapter3\"]\n",
    "for i in range(len(chapters)):\n",
    "    analyse_chapter_at(i)\n",
    "\n",
    "```\n",
    "\n",
    "But **YOU CAN HAVE THEM BOTH**\n",
    "\n",
    "Below, we use the `enumerate()` function. This returns **two values** stored in **two separate variables**. The first contains the index, and the second the actual object. \n",
    "\n",
    "Turns out you can have it all. \n",
    "\n",
    "Check out the code below and see if you can understand it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?ax.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which TFIDF scores would be useful when trying to determine which class?\n",
    "import matplotlib.pyplot as plt\n",
    "classes = [vectorised_space.todense(), vectorised_hockey.todense()]\n",
    "\n",
    "#Good features\n",
    "features = ['nhl', 'moon']\n",
    "\n",
    "#Bad features\n",
    "#features = ['article', 'subject']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "col = \"bo\"\n",
    "ax.set_xlabel(features[0])\n",
    "ax.set_ylabel(features[1])\n",
    "for i, c in enumerate(classes):\n",
    "    #Get TFIDF for all posts, for the column for each vocab\n",
    "    x = c[:, vocab == features[0]]\n",
    "    y = c[:, vocab == features[1]]\n",
    "    ax.plot(x, y, col, label = \"space\" if i == 0 else \"hockey\")\n",
    "    col = \"gx\"\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lots of values near 0\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "for i,c in enumerate(classes):\n",
    "    x = c[:, vocab == features[0]]\n",
    "    y = c[:, vocab == features[1]]\n",
    "    ax.set_xlim([0,0.5])\n",
    "    ax.set_yscale(\"log\")\n",
    "    \n",
    "    ax.hist(x, bins = 100)\n",
    "    ax.hist(y, bins = 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"../media/supervisedlearninglearner.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "Models don't have to be constructed with machine learning. The model is simply something that when given an **input**, which in our case waill be some vectorised text, it is able to produce some **output**. In the case of **classification** this will always be a single discrete label. \n",
    "\n",
    "This model could be made by hand, and in fact some text classification models were made by hand until quite recently. However, this can be time consuming and complicated, and result in models which are not particularly robust. \n",
    "\n",
    "Machine learning allows us to make **models** from **data**. This means that given **new data** that the model hasn't seen before, we can assign a new label to that new data. \n",
    "\n",
    "### Whats the simplest data-driven model?\n",
    "\n",
    "Lets think of the simplest model we can make, using the data we have. For example, we could get 50% accuracy by just picking the same class every time without even looking at the data. Generally, we would hope to be doing better than **mean** or chance!\n",
    "\n",
    "We can do much better than that using **learning algorithms** to generate our models in a process called **training**. We will cover a couple of learning algorithms used for classification later in the lecture. \n",
    "\n",
    "\n",
    "### Datasets\n",
    "\n",
    "The format of our dataset is quite simple. Each example (in our case each text document), is represented by 1 or more numbers. This might be a Bag of Words Vector, a TF/IDF vector, or a Topic Model Vector, or some other representations we'll learn about later on in the course. \n",
    "\n",
    "This is the **input**. Each example also has a label, or an **output**. This label tells us which class the example belongs to and we can use this to **train** our model.\n",
    "\n",
    "#### Labels\n",
    "A quick word on labels in classification tasks. They are:\n",
    "\n",
    "- Discrete \n",
    "- Categorical (not numerical, although we often use numbers). 1 is not less than 2, 3 is not half 6. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training \n",
    "\n",
    "This is where we take the dataset (the input that represents each example, and its accompanying label), and generate a model that will learn to take new inputs, and be able to correctly label them. \n",
    "\n",
    "The way this happens is often starting from some random initialisation, we iteratively try each example, check if we got it right by comparing the model's prediction against the actual label, and then use this information to improve the model. Eventually, we end up with a model that works well (hopefully!). \n",
    "\n",
    "The algorithms we'll look at today actually don't really have much of a training process as such, unlike the more fancy learning algorithms we'll look at later in the course. \n",
    "\n",
    "### New Data\n",
    "\n",
    "Now we've trained the model, its time to try it out on some new data! Remember, the goal is to make something that is able to correctly label new data (new instances of text) based on what it has learned about the categories from the dataset. It is worth noting here that models will only tend to be good at identifying new examples that are similar, or at are least different in similar ways, to the ones it has seen before.  \n",
    "\n",
    "We'll look into how to evaluate our models later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Pick features\n",
    "#features = [\"subject\",\"article\"]\n",
    "\n",
    "features = [\"moon\",\"nhl\"]\n",
    "#features = [\"space\",\"hockey\", \"moon\",\"nhl\"]\n",
    "# features = np.random.choice(vocab, 100)\n",
    "\n",
    "#Add features to main dataframe\n",
    "\n",
    "vectorised_dense = vectorised.todense()\n",
    "\n",
    "\n",
    "for f in features:\n",
    "    df[f] = vectorised_dense[:, vocab == f]\n",
    "#Initialise the model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "#Get the dataset\n",
    "X = df[features]\n",
    "y = df[\"label\"]\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "#Train the model\n",
    "model = gnb.fit(X, y)\n",
    "\n",
    "#See if the model works\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "#How many did it get right?\n",
    "num_incorrect = (y != y_pred).sum()\n",
    "total = df[\"label\"].shape[0]\n",
    "acc = (total - num_incorrect) / total * 100\n",
    "print(\"Number of mislabeled points out of a total %d points : %d, %0.3f\" % (total, num_incorrect, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good is that score?\n",
    "\n",
    "Is the model better than random? Here we show what happens if we randomly guess a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate some random results\n",
    "y_pred = np.random.randint(0,2,len(y))\n",
    "\n",
    "#How many did it get right?\n",
    "num_incorrect = (y != y_pred).sum()\n",
    "total = df[\"label\"].shape[0]\n",
    "acc = (total - num_incorrect) / total * 100\n",
    "print(\"Number of mislabeled points out of a total %d points : %d, %0.3f\" % (total, num_incorrect, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good for some....\n",
    "\n",
    "For some examples the two words are really good, but for some examples where the TF/IDF for each word is near 0, its a bad representation\n",
    "\n",
    "For example, out of 1000 of each category, we may have a 300 space emails where the features are (0,0), and 400 hockey emails where the features are (0,0). SO if we then have a new email that scores (0,0), what category do with put it in? We can't use just these features to reliably tell them apart!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the accuracy \n",
    "acc = (total - num_incorrect) / total * 100\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Models \n",
    "\n",
    "#### Training Set Accuracy\n",
    "Training set accuracy is when we take all the data we trained the model on and use it to determine the accuracy of that same trained model. It's important to remember that the goal of most traditional machine learning tasks is to take existing examples and produce something which generalises to new examples. \n",
    "\n",
    "In an issue known as overfitting, a model may have high training set accuracy, but when given new examples will perform poorly. In our text blog example, this means the model has learnt to identify the exact blog posts it has seen before, rather than learning something intrinsic about all space or hockey emails that would allow it to correctly label novel ones under different circumstances in the wild. \n",
    "\n",
    "#### Validation Set Accuracy\n",
    "To get a more accurate measure of accuracy (meta, right?), you can hold back some of your data from training and use this to evaluate your model during development. This way your model is being tested on unseen data and you can have more confidence it will work when you put it into production. Proportions for this split vary, but 10% is often used.\n",
    "\n",
    "The problem with test set accuracy is that you lose some of your precious data for training. Also, if you have small datasets, a 10% split of an already small number of examples may not actually give you much of an idea about how well your model is performing. \n",
    "\n",
    "You can see that no one method is right for all situations and compromises often have to be made. It's also important to note that these methods will only work as well as your data is good. They won’t, for example, spot bias in your model if your test set also lacks the necessary diversity. \n",
    "\n",
    "#### Test Sets\n",
    "Some people hold back even more data to really test how well their model will work in the real world (or \"production\"). This is because they may overfit to their testing data when tuning their models. \n",
    "\n",
    "#### Cross Validation \n",
    "Its the best (or worst?) of both worlds approach! More on that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `train_test_split(X, y, split)`\n",
    "\n",
    "There is a function in the `sklearn` library that we can use called `train_test_split()` we can use to create our different datasets for **training** and **validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add features to main dataframe\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorised_df, df[\"label\"], test_size=0.2, random_state=0)\n",
    "#Train the model\n",
    "model = gnb.fit(X_train,y_train)\n",
    "#See if the model works\n",
    "y_pred = model.predict(X_test)\n",
    "num_incorrect = (y_test != y_pred).sum()\n",
    "total = y_test.shape[0]\n",
    "acc = (total - num_incorrect) / total * 100\n",
    "print(\"Number of mislabeled points out of a total %d points : %d, Accuracy is %0.3f percent\" % (total, num_incorrect, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Look up the documentation using ?\n",
    "?train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Representations\n",
    "\n",
    "OK, so now we can compare TF/IDF, SVD and LDA representations, remember, one isn't better than the other for all datasets and problems. One might be better for this dataset and problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set things up\n",
    "lda_cv = CountVectorizer(stop_words='english', tokenizer=my_tokeniser,\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "count_data = lda_cv.fit_transform(df[\"text\"])\n",
    "lda = LatentDirichletAllocation(n_components=16,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics = lda.fit_transform(count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the 3 representations\n",
    "tfidf = np.asarray(vectorised.todense())\n",
    "features = [tfidf, svd_topic_vectors, lda_topics]\n",
    "i = 0\n",
    "for f in features:\n",
    "    print(\"Feature set \",i)\n",
    "    i = i+1\n",
    "    X_train, X_test, y_train, y_test = train_test_split(f, df[\"label\"], test_size=0.3, random_state=0)\n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "    num_incorrect = (y_test != y_pred).sum()\n",
    "    total = y_test.shape[0]\n",
    "    acc = (total - num_incorrect) / total * 100\n",
    "    print(\"Number of mislabeled points out of a total %d points : %d, %0.3f accurate\" % (total, num_incorrect, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Now we're going to learn about the algorithm that we've been using to classify, as it is one widely used in text classification tasks. \n",
    "\n",
    "When we see a new example we ask \"Given I know that this document has these feature values, what is the probability that it is from each class?\". Naive Bayes will then return the **most likely** label, given those values. \n",
    "\n",
    "```\n",
    "P(c=0|x1,x2,x3..)\n",
    "P(c=1|x1,x2,x3..)\n",
    "```\n",
    "\n",
    "How we reach this probability is in two steps.\n",
    "\n",
    "First, we ask \"What is the probability that a document in a particular class has these particular values?\". We can look at a **probability distribution** (as shown in the plot below) to work this out. For example, we may see that that for **space (class 0)**, the probability of a seeing a **TF/IDF** score of 0.4 for the word **year** might be 0.084 and the probability in **hockey (class 1)** is only 0.069. \n",
    "\n",
    "```\n",
    "P(time=0.4|c=0) = 0.084\n",
    "P(time=0.4|c=1) = 0.069\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats \n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,8))\n",
    "feature = \"time\"\n",
    "for i,c in enumerate(classes):\n",
    "    #Get vals for word\n",
    "    tfidf_vals = c[:, vocab == feature]\n",
    "    lim = 220\n",
    "    #Remove 0 scores (for plotting)\n",
    "    tfidf_vals = sorted(np.array(tfidf_vals.flatten())[0])[-lim:]\n",
    "    x = np.linspace(0, 0.2, len(tfidf_vals))\n",
    "    #Find mean and standard deviation\n",
    "    m, s = stats.norm.fit(tfidf_vals)\n",
    "    #Draw normal curve\n",
    "    pdf_g = stats.norm.pdf(x, m, s)\n",
    "    ax[i].set_ylim([0,25])\n",
    "    ax[i].hist(tfidf_vals, bins = 40)\n",
    "    ax[i].plot(x, pdf_g, label=\"Probabilty distribution\")\n",
    "    index = 43\n",
    "    print(\"prob = \", pdf_g[index]/len(tfidf_vals))\n",
    "    ax[i].plot(x[index], pdf_g[index],\"rx\",ms=12, label=\"tfxidf = 0.4\") \n",
    "    ax[i].set_xlabel(\"tfxidf scores for \\\"\" + feature +\"\\\"\")\n",
    "    ax[i].set_title(\"space\" if i == 0 else \"hockey\")\n",
    "    ax[i].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But this is not enough, because it may also be that **regardless** of observations, its actually way more likely that you'll get one class over another. \n",
    "\n",
    "In our case the classes are nicely balanced so this won't have much effect, because the chance of seeing each class, regardless of what we know about the text, is about 50/50\n",
    "\n",
    "But using **Bayes Rule** we can calculate the probability of each class, given the observation, taking into account the probability before you had made the observation. \n",
    "\n",
    "```\n",
    "P(c=0|year=0.1) = P(c=0) * P(year=0.1|c=0)\n",
    "P(c=1|year=0.1) = P(c=1) * P(year=0.1|c=1)\n",
    "```\n",
    "\n",
    "But this is only for one of the features, and as we've seen, we may use anywhere from 1 to thousands! \n",
    "\n",
    "We can calculate each probability for each feature separately and combine them to get a final probability and this is where the **naive** in **naive bayes** comes in. \n",
    "\n",
    "The maths that we use to combine the probabilities assumes that all the features are independent and unrelated, however, it is quite likely that some will actually be related in some way. We ignore this assumption and use it anyway and it turns out it sitll works quite well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Classification only works for what you trained it for \n",
    "\n",
    "1. Will always make pick one of the classes its been trained on \n",
    "\n",
    "\n",
    "2. Data has to be in the same format \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses of Text Classification in Creative Industries \n",
    "\n",
    "- Classifying social media response to events \n",
    "\n",
    "\n",
    "    - Did people enjoy the show?\n",
    "\n",
    "\n",
    "- Grouping media from text descriptions \n",
    "\n",
    "\n",
    "    - Can help if we have categories that we know are useful\n",
    "\n",
    "\n",
    "- Identifying new items in museum collections \n",
    "\n",
    "\n",
    "    - Which experts should look at these documents?\n",
    "\n",
    "\n",
    "- Filtering out inappropriate content \n",
    "\n",
    "\n",
    "    - Live performances with audience participation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules of Thumb\n",
    "\n",
    " - A model is created by a learning algorithm \n",
    " \n",
    " \n",
    " - A classifier learns to attach discrete labels to new data\n",
    " \n",
    " \n",
    " - We want a model that works well with new data (generalises well, **not overfit**)\n",
    " \n",
    " \n",
    " - Different features can effect model performance\n",
    " \n",
    " \n",
    " - The model can only learn variations similar to that it has seen before  \n",
    "     \n",
    "     \n",
    "     - Generally less features is better\n",
    "     \n",
    "     \n",
    "     - Generally more data is better, especially for more complex problems \n",
    "     \n",
    "     \n",
    " - More classes (more choices), generally is a harder problem \n",
    " \n",
    " \n",
    " - Equal numbers of examples for each class is also generally better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
