{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8: Word Vectors\n",
    "\n",
    "In this weeks sesssion we are going to be looking at word vectors. Word vectors (or *word embeddings*) are pretrained numerical representations of words, within a high-dimensional vector space. The number of dimensions for our word vectors is arbitrary and can range anywhere from 50 to 300 dimensions. \n",
    "\n",
    "Word vectors are calculated from very large datasets of texts, with the goal of words that are similiar being close to each other in vector space, and words being dissimilar being far away in vector space. After processing a vast amount of data, we end up with a unique vector for every word in the corpus. This gives us feature representations of words (that unlike other representations of words we have seen like one-hot, bag of words or TF-IDF) encode a representation that captures the meaning of the word. \n",
    "\n",
    "As these word vectors are **numerical representations**, we can perform mathematical functions on them to do some interesting (and revealing) insights into what kind of data and biases these models contain. \n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext.vocab as vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download word vectors\n",
    "\n",
    "Here we are going to load our set of word vectors using the torchtext library. Here we are downloading the [GloVe pretrained word embeddings](https://nlp.stanford.edu/projects/glove/) trained a data dump of Wikipedia from 2014. There are [other pretrained word embeddings](https://torchtext.readthedocs.io/en/latest/vocab.html#pretrained-word-embeddings) available in torchtext. You can try loading in other ones later and see how that effects results. \n",
    "\n",
    "This download is about 1GB. You should **run this before the class**. If you haven't done this before the class for whatever reason and it is taking too long to download in class then kill the cell (or restart the kernel) and instead use the function in the cell following the next one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = vocab.GloVe(name=\"6B\",dim=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Alternative) load a sub-sample of word vectors\n",
    "\n",
    "If the previous cell is taking too long to download, you can uncomment this line to load in a sample of the top 30K word vectors from GloVe to use for this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_vectors = vocab.Vectors(name = '../data/glove.6B.100d.top30k.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look of one of our word vectors. It looks like a big list of numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3082,  0.3094,  0.5280, -0.9254, -0.7367,  0.6348,  0.4420,  0.1026,\n",
       "        -0.0914, -0.5661, -0.5327,  0.2013,  0.7704, -0.1398,  0.1373,  1.1128,\n",
       "         0.8930, -0.1787, -0.0020,  0.5729,  0.5948,  0.5043, -0.2899, -1.3491,\n",
       "         0.4276,  1.2748, -1.1613, -0.4108,  0.0428,  0.5487,  0.1890,  0.3759,\n",
       "         0.5803,  0.6697,  0.8116,  0.9386, -0.5100, -0.0701,  0.8282, -0.3535,\n",
       "         0.2109, -0.2441, -0.1655, -0.7836, -0.4848,  0.3897, -0.8636, -0.0164,\n",
       "         0.3198, -0.4925, -0.0694,  0.0189, -0.0983,  1.3126, -0.1212, -1.2399,\n",
       "        -0.0914,  0.3529,  0.6464,  0.0896,  0.7029,  1.1244,  0.3864,  0.5208,\n",
       "         0.9879,  0.7995, -0.3462,  0.1409,  0.8017,  0.2099, -0.8601, -0.1531,\n",
       "         0.0745,  0.4082,  0.0192,  0.5159, -0.3443, -0.2453, -0.7798,  0.2743,\n",
       "         0.2242,  0.2016,  0.0174, -0.0147, -1.0235, -0.3970, -0.0056,  0.3057,\n",
       "         0.3175,  0.0214,  0.1184, -0.1132,  0.4246,  0.5340, -0.1672, -0.2718,\n",
       "        -0.6255,  0.1288,  0.6253, -0.5209])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7574,  0.1693, -0.7839, -0.1091,  0.0082,  0.7234,  1.4583, -0.0700,\n",
       "         0.0121, -0.1024, -0.4730, -0.3713, -0.1743,  0.9255,  0.5588,  0.2687,\n",
       "         0.5315, -0.8269,  0.0700, -0.1635, -0.4140,  0.8368, -0.3771, -0.3150,\n",
       "        -0.1433,  1.3757,  0.2553, -0.8395, -0.4538, -0.6820,  0.7295,  0.6717,\n",
       "        -0.2971, -0.7698, -0.1653,  0.6540,  0.3992,  0.4613,  0.1260, -1.4694,\n",
       "         0.9445, -1.7318, -0.4817, -1.0355,  0.1341,  0.4327, -0.2064,  0.0087,\n",
       "         0.6242, -0.9442, -0.2482, -0.3284, -0.1797,  1.2036, -0.8806, -1.0946,\n",
       "        -0.4835,  0.7340,  0.5827,  0.3725,  0.6041,  0.4534,  0.0388, -0.1667,\n",
       "         0.2082, -0.5358,  0.6453, -0.1996, -0.0616, -0.8759, -0.2334, -0.0343,\n",
       "        -0.0174,  0.6223,  0.6372,  0.8106,  0.4091, -0.8603,  0.8655, -0.0143,\n",
       "         0.1666, -0.4490, -0.2643,  1.0010, -0.1944, -0.8739,  0.3933,  0.0464,\n",
       "         0.3095, -0.0749, -0.0024,  0.1525, -1.1183, -0.5085, -0.3071, -1.1481,\n",
       "        -0.5662,  0.0923,  1.0424,  0.3507])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['cream']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On their own, these word vectors are not particularly meaningful. No person looking a this would not be able to make sense of it's meaning. \n",
    "\n",
    "Where word vectors become powerful is when we make comparisons between them. We can use the [cosine similarity](https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html) function in PyTorch to get a measure of similarity between our two vectors. \n",
    "\n",
    "As this is a similarity measurement, the higher the value the most similar. 1 is the highest value we can get and 0 is the lowest value. Lets compare our word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words dog and dog have a cosine similiarity of 1.000000\n",
      "The words dog and phone have a cosine similiarity of 0.302810\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = nn.CosineSimilarity(dim=0)\n",
    "\n",
    "similarity = cosine_sim(word_vectors['dog'], word_vectors['dog'])\n",
    "print(f'The words dog and dog have a cosine similiarity of {similarity.item():3f}')\n",
    "\n",
    "similarity = cosine_sim(word_vectors['dog'], word_vectors['phone'])\n",
    "print(f'The words dog and phone have a cosine similiarity of {similarity.item():3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare some more words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words dog and fox have a cosine similiarity of 0.414250\n",
      "The words cat and fox have a cosine similiarity of 0.393508\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_sim(word_vectors['dog'], word_vectors['fox'])\n",
    "print(f'The words dog and fox have a cosine similiarity of {similarity.item():3f}')\n",
    "similarity = cosine_sim( word_vectors['cat'], word_vectors['fox'])\n",
    "print(f'The words cat and fox have a cosine similiarity of {similarity.item():3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foxes are in the canine family so this is accurate! \n",
    "\n",
    "Now lets compare London to some cities around the world:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words london and paris have a cosine similiarity of 0.733768\n",
      "The words london and madrid have a cosine similiarity of 0.481906\n",
      "The words london and beirut have a cosine similiarity of 0.396094\n",
      "The words london and beijing have a cosine similiarity of 0.454936\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_sim(word_vectors['london'], word_vectors['paris'])\n",
    "print(f'The words london and paris have a cosine similiarity of {similarity.item():3f}')\n",
    "similarity = cosine_sim(word_vectors['london'], word_vectors['madrid'])\n",
    "print(f'The words london and madrid have a cosine similiarity of {similarity.item():3f}')\n",
    "similarity = cosine_sim(word_vectors['london'], word_vectors['beirut'])\n",
    "print(f'The words london and beirut have a cosine similiarity of {similarity.item():3f}')\n",
    "similarity = cosine_sim(word_vectors['london'], word_vectors['beijing'])\n",
    "print(f'The words london and beijing have a cosine similiarity of {similarity.item():3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And cities in the UK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words london and edinburgh have a cosine similiarity of 0.683086\n",
      "The words london and glasgow have a cosine similiarity of 0.669558\n",
      "The words glasgow and edinburgh have a cosine similiarity of 0.840325\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_sim(word_vectors['london'], word_vectors['edinburgh'])\n",
    "print(f'The words london and edinburgh have a cosine similiarity of {similarity.item():3f}')\n",
    "similarity = cosine_sim(word_vectors['london'], word_vectors['glasgow'])\n",
    "print(f'The words london and glasgow have a cosine similiarity of {similarity.item():3f}')\n",
    "similarity = cosine_sim(word_vectors['glasgow'], word_vectors['edinburgh'])\n",
    "print(f'The words glasgow and edinburgh have a cosine similiarity of {similarity.item():3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And cities in Ireland:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words london and dublin have a cosine similiarity of 0.691274\n",
      "The words london and belfast have a cosine similiarity of 0.603107\n",
      "The words dublin and belfast have a cosine similiarity of 0.797412\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_sim(word_vectors['london'], word_vectors['dublin'])\n",
    "print(f'The words london and dublin have a cosine similiarity of {similarity.item():3f}')\n",
    "similarity = cosine_sim(word_vectors['london'], word_vectors['belfast'])\n",
    "print(f'The words london and belfast have a cosine similiarity of {similarity.item():3f}')\n",
    "similarity = cosine_sim(word_vectors['dublin'], word_vectors['belfast'])\n",
    "print(f'The words dublin and belfast have a cosine similiarity of {similarity.item():3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure distances on your own words\n",
    "\n",
    "Try putting your own words in here to see the distance scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These words have a distance of: 0.000000\n"
     ]
    }
   ],
   "source": [
    "word1 = ''\n",
    "word2 = ''\n",
    "similarity = cosine_sim(word_vectors[word1], word_vectors[word2])\n",
    "print(f'These words have a distance of: {similarity.item():3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding closest words\n",
    "\n",
    "The following function will let us look for the closest words in vector space to a target word. The following function calculates this using the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) instead of the Cosine Similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is sourced from: https://www.cs.toronto.edu/~lczhang/321/lec/glove_notes.html\n",
    "def print_closest_words(vec, n=5):\n",
    "    dists = torch.norm(word_vectors.vectors - vec, dim=1)     # compute distances to all words\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance\n",
    "    for idx, difference in lst[1:n+1]: \t\t\t\t\t       # take the top n\n",
    "        print(word_vectors.itos[idx], difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat 2.681131\n",
      "dogs 3.2425272\n",
      "puppy 3.950055\n",
      "pet 3.9634414\n",
      "horse 4.328852\n",
      "pig 4.4629855\n",
      "cats 4.518958\n",
      "animal 4.5231004\n",
      "rabbit 4.547051\n",
      "boy 4.598282\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(word_vectors[\"dog\"], n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sydney 4.234696\n",
      "paris 4.6191545\n",
      "melbourne 4.6299014\n",
      "dublin 4.6676564\n",
      "edinburgh 4.843591\n",
      "glasgow 4.863154\n",
      "york 4.871911\n",
      "opened 5.031126\n",
      "birmingham 5.0848064\n",
      "amsterdam 5.094095\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(word_vectors[\"london\"], n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hornsey 3.5537856\n",
      "islington 3.5999157\n",
      "croydon 3.648258\n",
      "plaistow 3.7073412\n",
      "shoreditch 3.7357748\n",
      "coulsdon 3.8395576\n",
      "eltham 3.9164743\n",
      "highgate 3.939703\n",
      "southfields 3.9632952\n",
      "beckenham 3.971325\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(word_vectors[\"camberwell\"], n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potatoes 3.9128337\n",
      "peanut 3.9875195\n",
      "tomato 4.1266975\n",
      "bean 4.1955256\n",
      "pumpkin 4.199014\n",
      "baked 4.2010217\n",
      "bread 4.3381357\n",
      "fried 4.392579\n",
      "toast 4.4026785\n",
      "mashed 4.424257\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(word_vectors[\"potato\"], n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physician 3.6094282\n",
      "nurse 3.8185012\n",
      "patient 4.220124\n",
      "dentist 4.256469\n",
      "dr. 4.27268\n",
      "surgeon 4.3080616\n",
      "psychiatrist 4.4265003\n",
      "doctors 4.4447513\n",
      "colleague 4.538765\n",
      "pharmacist 4.575756\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(word_vectors[\"doctor\"], n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try putting your own words into this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase 2.7446012\n",
      "meaning 2.9835567\n",
      "words 3.1709633\n",
      "name 3.6496246\n",
      "literally 3.9742608\n",
      "referred 4.0050144\n",
      "refer 4.006663\n",
      "refers 4.013667\n",
      "simply 4.051273\n",
      "instance 4.0517354\n"
     ]
    }
   ],
   "source": [
    "my_word = 'word'\n",
    "print_closest_words(word_vectors[my_word], n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing arithmetic on word vectors\n",
    "\n",
    "We can do arithmetic on word vectors to create new vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1023, -0.8129,  0.1021,  0.9859,  0.3422,  1.0910, -0.4891, -0.0562,\n",
       "        -0.2103, -1.0300, -0.8685,  0.3679,  0.0196,  0.5926, -0.2319, -1.0169,\n",
       "        -0.0122, -1.1719, -0.5233,  0.6065, -0.9854, -1.0010,  0.4891,  0.6301,\n",
       "         0.5822,  0.1591,  0.4369, -1.2535,  0.9705, -0.0655,  0.7338,  0.4422,\n",
       "         1.2092,  0.1970, -0.1595,  0.3436, -0.4622,  0.3377,  0.1479, -0.2496,\n",
       "        -0.7709,  0.5227, -0.1283, -0.9188, -0.0176, -0.4404, -0.5266,  0.3373,\n",
       "         0.6064, -0.4507, -0.0416,  0.0841,  1.3146,  0.6774, -0.2432, -2.0710,\n",
       "        -0.6065,  0.1971,  0.6357,  0.0782,  0.4916,  0.0817,  0.7086,  0.2019,\n",
       "         0.5156, -0.2303, -0.4047,  0.3921, -0.5093, -0.1392,  0.2161, -0.6287,\n",
       "         0.0889,  0.4917, -0.0664,  0.7610, -0.1944,  0.4113, -1.0448, -0.1480,\n",
       "        -0.0984, -0.2512,  0.8090,  0.3631, -0.7820, -0.1048,  0.0834, -1.2407,\n",
       "         0.6553, -0.9363,  0.6484, -0.5583,  0.4562,  0.2758, -1.5490, -0.1991,\n",
       "        -0.5080, -0.1382,  0.2773, -0.7572])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_vector = word_vectors['king'] - word_vectors['man'] + word_vectors['woman']\n",
    "new_word_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, this is not very interpretable. But we can use mathematical functions to learn more about the new word vectors we have created: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our new vector has a cosine similarity of 0.393379 to the word man\n",
      "Our new vector has a cosine similarity of 0.557549 to the word woman\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_sim(new_word_vector, word_vectors['man'])\n",
    "print(f'Our new vector has a cosine similarity of {similarity.item():3f} to the word man')\n",
    "similarity = cosine_sim(new_word_vector, word_vectors['woman'])\n",
    "print(f'Our new vector has a cosine similarity of {similarity.item():3f} to the word woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use our our search function to find the closest word vectors to our new word in vector space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen 4.081079\n",
      "monarch 4.6429076\n",
      "throne 4.9055004\n",
      "elizabeth 4.921559\n",
      "prince 4.981147\n",
      "daughter 4.985715\n",
      "mother 5.0640874\n",
      "cousin 5.077497\n",
      "princess 5.0786853\n",
      "widow 5.1283097\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(new_word_vector, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating bias in word vectors\n",
    "\n",
    "Now lets use these tools to see expose the biases encoded in word vectors. \n",
    "\n",
    "If we subtract man from the word doctor, and add the vector for woman, the closest word vectors are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nurse 4.2283154\n",
      "physician 4.7054324\n",
      "woman 4.8734255\n",
      "dentist 4.969891\n",
      "pregnant 5.014848\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(word_vectors['doctor'] - word_vectors['man'] + word_vectors['woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However when we subtract woman from doctor and add man, we do not get the same effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man 4.8998694\n",
      "dr. 5.05853\n",
      "brother 5.144743\n",
      "physician 5.1525483\n",
      "taken 5.2571893\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(word_vectors['doctor'] - word_vectors['woman'] + word_vectors['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However when we do the same thing with the word nurse, then we do get the word doctor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doctor 4.2283154\n",
      "technician 4.7353873\n",
      "sergeant 4.775118\n",
      "physician 4.786661\n",
      "paramedic 4.8385634\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(word_vectors['nurse'] - word_vectors['woman'] + word_vectors['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try investigating your own words for bias:\n",
    "\n",
    "Plug in different words here and investigate your own kinds of bias. It does not have to be gender bias, it could be racial, class, sexuality, disability or other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undergraduate 5.0620193\n",
      "graduate 5.3035765\n",
      "student 5.346771\n",
      "faculty 5.43235\n",
      "educational 5.6167006\n"
     ]
    }
   ],
   "source": [
    "original_word = 'academic'\n",
    "negative_word = 'man'\n",
    "positive_word = 'woman'\n",
    "new_word_vector = word_vectors[original_word] - word_vectors[negative_word] + word_vectors[positive_word]\n",
    "print_closest_words(new_word_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a go at [loading in](#download-word-vectors) some of the [different word vectors available on torchtext](https://torchtext.readthedocs.io/en/latest/vocab.html#pretrained-word-embeddings), or using a different dimensionality for the GloVe vectors and re-run the cells in this notebook. How does that impact the results? (You may want to make a copy of this notebook to make a side-by-side comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44a0508e6db17bfb5aee65ed6759df003938501ba24285fe2dbd7c31574fa72a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
