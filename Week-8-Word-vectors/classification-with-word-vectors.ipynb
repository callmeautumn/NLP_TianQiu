{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classifier using word vector features\n",
    "\n",
    "\n",
    "In this notebook we will look at how to train a neural network text classifier in PyTorch. For this we will be using word vectors as the input features for our neural network. \n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case it's not installed\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext.vocab as vocab\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.distributions import Categorical\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define hyperparameters\n",
    "\n",
    "Now lets define out hyperparameters. It is important to set `num_classes` correctly for however may classes there are in your dataset. \n",
    "\n",
    "`max_tokens` defines how many tokens we look at at once. If our text document exceeds this number it will be clipped by `max_tokens`. If our document has less tokens then the input matrix to the model will be zero padded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "max_tokens = 200\n",
    "hidden_dim = 256\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "num_epochs = 101\n",
    "learning_rate = 0.002\n",
    "load_chk = False    # load in pre-trained checkpoint for training\n",
    "save_path = \"wordvec_classifier_model.pt\"\n",
    "# load_path = \"wordvec_classifier_model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) pre-process data\n",
    "\n",
    "You don't need to run the next cell, as it has already been done for you. \n",
    "\n",
    "It takes the raw Myers Briggs dataset `data/myers_briggs_comments.tsv` and pre-processes it using stop words and lemmatisation, and gives class labels for the code. Using the same preprocessing steps as are in the Week 6 notebooks. \n",
    "\n",
    "Here we are dividing the data into 2 classes, if you want to change it you can edit the file `data-util/preprocess_myersbriggs.py`, run this cell, and create a different class label mapping for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python data-util/preprocess_myersbriggs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load word vectors\n",
    "\n",
    "Now lets load our word vectors. If this is taking too long to download in class you can change the first line to:\n",
    "\n",
    "```word_vectors = vocab.Vectors(name = '../data/glove.6B.100d.top30k.txt')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = vocab.GloVe(name=\"6B\",dim=100) \n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "wordvec_embeddings = nn.Embedding.from_pretrained(word_vectors.vectors)\n",
    "embedding_dim = wordvec_embeddings.weight.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset class\n",
    "\n",
    "Now lets create a dataset class for our tab-seperated values (TSV) files. This will tell pytorch how to load and sample our dataset during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSVDataset(Dataset):\n",
    "    def __init__(self, tsv_file, transform=None):\n",
    "        self.data = pd.read_csv(tsv_file, sep='\\t')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx, 0]\n",
    "        label = self.data.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            text = self.transform(text)\n",
    "\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to word vector matrix\n",
    "\n",
    "This class will take a text string and return a matrix containing a sequence of word vectors. This is what will be inputted into our model. Here our model takes inputs of a fixed length so we need to normalise our data so that every input matrix is the same size, regardless of the length of the original text.\n",
    "\n",
    "Here we start with an empty matrix of zeros, and then put in our word vectors into the rows of the matrix, stopping once we reach the length of the matrix (defined by `max_rows`). \n",
    "\n",
    "At the end we have a matrix of shape (`max_tokens`, `embedding_dim`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wordvec_tensor(input_str, max_tokens, embedding_dim):\n",
    "    # Create empty tensor of zeros \n",
    "    output_tensor = torch.zeros(max_tokens, embedding_dim)\n",
    "\n",
    "    # Get tokens\n",
    "    tokens = tokenizer(str(input_str))\n",
    "\n",
    "    # Make sure that there are tokens in the list before doing the next steps\n",
    "    if tokens != []:\n",
    "        # Clip tokens to the token windown length\n",
    "        tokens = tokens[:max_tokens]\n",
    "\n",
    "        # Get word vectors from tokens\n",
    "        wordvec_seq = word_vectors.get_vecs_by_tokens(tokens)\n",
    "\n",
    "        # Fill empty_tensor with the values from x\n",
    "        output_tensor[:wordvec_seq.shape[0], :wordvec_seq.shape[1]] = wordvec_seq\n",
    "        \n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create our test and train set classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([lambda x: extract_wordvec_tensor(x, max_tokens, embedding_dim)])\n",
    "\n",
    "train_set = TSVDataset('../data/mb_processed_train.tsv', transform=transform)\n",
    "test_set = TSVDataset('../data/mb_processed_test.tsv', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the classifier network\n",
    "\n",
    "Lets define our text classification model. Here we are just using two fully connected layers (defined by `nn.Linear`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, token_length, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(token_length * embedding_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc_1(x)\n",
    "        x = self.fc_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up network and optimiser\n",
    "\n",
    "Here we are creating an instance (`model`) our classier network class. As well as our loss function (`criterion`) and our optimiser (`optimizer`). \n",
    "\n",
    "We also define data loaders for our test and training sets, which give us objects we can iterate on in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier(max_tokens, embedding_dim, hidden_dim, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "if load_chk:\n",
    "    checkpoint = torch.load(load_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "\n",
    "Here we train the classifier. We cycle through the dataset in its entirery (an epoch) for however many training epochs we have defined in `num_epochs`. \n",
    "\n",
    "Every 10 epochs, we will test the model on the test set. When the model starts to perform worse on the test set (even if training loss is going down), then we can assume the model is **overfitting** to the training data, and there we should probably stop training. The code here automatically checks for this and will only save the model weights if it is an improvement on the test accuracy to the previous best checkpoint of the model. However traning will continue. \n",
    "\n",
    "As an educational exercise it is interesting to observe how the training and test loss increasingly diverge as the model starts to overfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_score = 100\n",
    "is_saved = False\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        text_batch, label_batch = batch\n",
    "        pred = model(text_batch)\n",
    "        probs = F.softmax(pred, dim=1)\n",
    "        loss = criterion(probs, label_batch)\n",
    "        running_loss += loss / (len(train_loader))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f'loss after epoch {epoch:03} is {running_loss:3f}')\n",
    "\n",
    "    if epoch % 10 == 0 and epoch > 0:\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0\n",
    "            for i, batch in enumerate(test_loader):\n",
    "                    model.zero_grad()\n",
    "                    text_batch, label_batch = batch\n",
    "                    pred = model(text_batch)\n",
    "                    probs = F.softmax(pred, dim=1)\n",
    "                    loss = criterion(pred, label_batch)\n",
    "                    running_loss += loss / (len(test_loader))\n",
    "            print(f'test loss is after epoch {epoch:03} is {running_loss:3f}')\n",
    "            if ((running_loss < best_test_score) or (not is_saved)):\n",
    "                 best_test_score = running_loss\n",
    "                 save_dict = {}\n",
    "                 save_dict['state_dict'] = model.state_dict()\n",
    "                 torch.save(save_dict, save_path)\n",
    "                 is_saved = True\n",
    "            else:\n",
    "                 print('test accuracy is getting worse, you may want to stop training now')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load best checkpoint\n",
    "\n",
    "Now lets reload our best checkpoint to evalaute our best model with the sklearn classification report that we used in Week 6. This way we can get a direct comparison between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell will give you an error if the state_dict dictionary hasn't been saved, \n",
    "#i.e. if you haven't run at least 10 epochs above\n",
    "checkpoint = torch.load(save_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load test set dataframes\n",
    "\n",
    "Here we need to load the test set as a pandas dataframe to get our variables `y_test` and `class_names` needed for the classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_df = pd.read_csv('../data/mb_processed_test.tsv', sep='\\t')\n",
    "y_test = test_set_df['1'].to_numpy()\n",
    "\n",
    "class_names_df = pd.read_csv('../data/mb_class_labels.tsv', sep='\\t')\n",
    "class_names = list(class_names_df['class_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get classification report\n",
    "\n",
    "This code will iterate over the test set one last time. This time getting predictions for each data sample and using the [sci-kit learn classificiaton report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to give us a break down of the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    for i, batch in enumerate(test_loader):\n",
    "            model.zero_grad()\n",
    "            text_batch, label_batch = batch\n",
    "            pred = model(text_batch)\n",
    "            probs = F.softmax(pred, dim=1)\n",
    "            prob_dist = Categorical(probs)\n",
    "            pred_classes = prob_dist.sample()\n",
    "            preds += pred_classes.numpy().tolist()\n",
    "    y_pred = np.array(preds)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=class_names)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus tasks\n",
    "\n",
    "- **Task A:** Try changing some of [the hyperparameters](#define-hyperparameters), like `max_tokens`, `hidden_dim`, `learning_rate` or `batch_size` and retraining the network. Has that improved the classification accuracy?\n",
    "- **Task B:** Try adding more layers to the network in [the cell that defines the classifier](#define-the-classifier-network). Does that improve the classification accuracy?\n",
    "- **Task C:** Try changing the preprocessing of the dataset to classify all 16 personality types in `data-util/preprocess_myersbriggs.py` and running the code in [the data pre-processing cell](#optional-pre-process-data). How does the classification accuracy look now?\n",
    "- **Task D:** Adapt this code to use a different dataset. You may need to write your own preprocessing script for this. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44a0508e6db17bfb5aee65ed6759df003938501ba24285fe2dbd7c31574fa72a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
