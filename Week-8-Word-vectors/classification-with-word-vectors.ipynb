{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classifier using word vector features\n",
    "\n",
    "\n",
    "In this notebook we will look at how to train a neural network text classifier in PyTorch. For this we will be using word vectors as the input features for our neural network. \n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Obtaining dependency information for torchtext from https://files.pythonhosted.org/packages/c2/5c/548948e239534b1f5fa4f09c246a97efdc62bfdf9bd460d363dece847829/torchtext-0.16.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading torchtext-0.16.1-cp39-cp39-win_amd64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torchtext) (4.66.1)\n",
      "Requirement already satisfied: requests in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.1 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torchtext) (2.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torchtext) (1.26.1)\n",
      "Collecting torchdata==0.7.1 (from torchtext)\n",
      "  Obtaining dependency information for torchdata==0.7.1 from https://files.pythonhosted.org/packages/08/05/d717b62841b32c29aabfb834d7fe606fdeb0420953b0391da1cde7804577/torchdata-0.7.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading torchdata-0.7.1-cp39-cp39-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torch==2.1.1->torchtext) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torch==2.1.1->torchtext) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torch==2.1.1->torchtext) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torch==2.1.1->torchtext) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torch==2.1.1->torchtext) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torch==2.1.1->torchtext) (2023.10.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from requests->torchtext) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from requests->torchtext) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from jinja2->torch==2.1.1->torchtext) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from sympy->torch==2.1.1->torchtext) (1.3.0)\n",
      "Downloading torchtext-0.16.1-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.9 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.7/1.9 MB 11.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.7/1.9 MB 11.2 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.7/1.9 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.9/1.9 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.4/1.9 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 6.9 MB/s eta 0:00:00\n",
      "Downloading torchdata-0.7.1-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 0.8/1.3 MB 26.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.0/1.3 MB 12.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 12.0 MB/s eta 0:00:00\n",
      "Installing collected packages: torchdata, torchtext\n",
      "Successfully installed torchdata-0.7.1 torchtext-0.16.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#In case it's not installed\n",
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext.vocab as vocab\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.distributions import Categorical\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define hyperparameters\n",
    "\n",
    "Now lets define out hyperparameters. It is important to set `num_classes` correctly for however may classes there are in your dataset. \n",
    "\n",
    "`max_tokens` defines how many tokens we look at at once. If our text document exceeds this number it will be clipped by `max_tokens`. If our document has less tokens then the input matrix to the model will be zero padded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "max_tokens = 200\n",
    "hidden_dim = 256\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "num_epochs = 101\n",
    "learning_rate = 0.002\n",
    "load_chk = False    # load in pre-trained checkpoint for training\n",
    "save_path = \"wordvec_classifier_model.pt\"\n",
    "# load_path = \"wordvec_classifier_model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) pre-process data\n",
    "\n",
    "You don't need to run the next cell, as it has already been done for you. \n",
    "\n",
    "It takes the raw Myers Briggs dataset `data/myers_briggs_comments.tsv` and pre-processes it using stop words and lemmatisation, and gives class labels for the code. Using the same preprocessing steps as are in the Week 6 notebooks. \n",
    "\n",
    "Here we are dividing the data into 2 classes, if you want to change it you can edit the file `data-util/preprocess_myersbriggs.py`, run this cell, and create a different class label mapping for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python data-util/preprocess_myersbriggs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load word vectors\n",
    "\n",
    "Now lets load our word vectors. If this is taking too long to download in class you can change the first line to:\n",
    "\n",
    "```word_vectors = vocab.Vectors(name = '../data/glove.6B.100d.top30k.txt')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [03:18, 4.34MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:11<00:00, 34689.08it/s]\n"
     ]
    }
   ],
   "source": [
    "word_vectors = vocab.GloVe(name=\"6B\",dim=100) \n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "wordvec_embeddings = nn.Embedding.from_pretrained(word_vectors.vectors)\n",
    "embedding_dim = wordvec_embeddings.weight.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset class\n",
    "\n",
    "Now lets create a dataset class for our tab-seperated values (TSV) files. This will tell pytorch how to load and sample our dataset during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSVDataset(Dataset):\n",
    "    def __init__(self, tsv_file, transform=None):\n",
    "        self.data = pd.read_csv(tsv_file, sep='\\t')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx, 0]\n",
    "        label = self.data.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            text = self.transform(text)\n",
    "\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to word vector matrix\n",
    "\n",
    "This class will take a text string and return a matrix containing a sequence of word vectors. This is what will be inputted into our model. Here our model takes inputs of a fixed length so we need to normalise our data so that every input matrix is the same size, regardless of the length of the original text.\n",
    "\n",
    "Here we start with an empty matrix of zeros, and then put in our word vectors into the rows of the matrix, stopping once we reach the length of the matrix (defined by `max_rows`). \n",
    "\n",
    "At the end we have a matrix of shape (`max_tokens`, `embedding_dim`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wordvec_tensor(input_str, max_tokens, embedding_dim):\n",
    "    # Create empty tensor of zeros \n",
    "    output_tensor = torch.zeros(max_tokens, embedding_dim)\n",
    "\n",
    "    # Get tokens\n",
    "    tokens = tokenizer(str(input_str))\n",
    "\n",
    "    # Make sure that there are tokens in the list before doing the next steps\n",
    "    if tokens != []:\n",
    "        # Clip tokens to the token windown length\n",
    "        tokens = tokens[:max_tokens]\n",
    "\n",
    "        # Get word vectors from tokens\n",
    "        wordvec_seq = word_vectors.get_vecs_by_tokens(tokens)\n",
    "\n",
    "        # Fill empty_tensor with the values from x\n",
    "        output_tensor[:wordvec_seq.shape[0], :wordvec_seq.shape[1]] = wordvec_seq\n",
    "        \n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create our test and train set classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([lambda x: extract_wordvec_tensor(x, max_tokens, embedding_dim)])\n",
    "\n",
    "train_set = TSVDataset(\"C:/Users/ROG/Documents/GitHub/NLP-23-24/data/mb_processed_test.tsv\", transform=transform)\n",
    "test_set = TSVDataset(\"C:/Users/ROG/Documents/GitHub/NLP-23-24/data/mb_processed_test.tsv\", transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the classifier network\n",
    "\n",
    "Lets define our text classification model. Here we are just using two fully connected layers (defined by `nn.Linear`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, token_length, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(token_length * embedding_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc_1(x)\n",
    "        x = self.fc_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up network and optimiser\n",
    "\n",
    "Here we are creating an instance (`model`) our classier network class. As well as our loss function (`criterion`) and our optimiser (`optimizer`). \n",
    "\n",
    "We also define data loaders for our test and training sets, which give us objects we can iterate on in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier(max_tokens, embedding_dim, hidden_dim, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "if load_chk:\n",
    "    checkpoint = torch.load(load_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "\n",
    "Here we train the classifier. We cycle through the dataset in its entirery (an epoch) for however many training epochs we have defined in `num_epochs`. \n",
    "\n",
    "Every 10 epochs, we will test the model on the test set. When the model starts to perform worse on the test set (even if training loss is going down), then we can assume the model is **overfitting** to the training data, and there we should probably stop training. The code here automatically checks for this and will only save the model weights if it is an improvement on the test accuracy to the previous best checkpoint of the model. However traning will continue. \n",
    "\n",
    "As an educational exercise it is interesting to observe how the training and test loss increasingly diverge as the model starts to overfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after epoch 000 is 0.566540\n",
      "loss after epoch 001 is 0.565453\n",
      "loss after epoch 002 is 0.563738\n",
      "loss after epoch 003 is 0.563130\n",
      "loss after epoch 004 is 0.563147\n",
      "loss after epoch 005 is 0.562308\n",
      "loss after epoch 006 is 0.562376\n",
      "loss after epoch 007 is 0.563124\n",
      "loss after epoch 008 is 0.563252\n",
      "loss after epoch 009 is 0.562130\n",
      "loss after epoch 010 is 0.561858\n",
      "test loss is after epoch 010 is 50.316326\n",
      "loss after epoch 011 is 0.561838\n",
      "loss after epoch 012 is 0.561145\n",
      "loss after epoch 013 is 0.561772\n",
      "loss after epoch 014 is 0.561947\n",
      "loss after epoch 015 is 0.563729\n",
      "loss after epoch 016 is 0.561664\n",
      "loss after epoch 017 is 0.560867\n",
      "loss after epoch 018 is 0.561087\n",
      "loss after epoch 019 is 0.560459\n",
      "loss after epoch 020 is 0.561253\n",
      "test loss is after epoch 020 is 81.617592\n",
      "test accuracy is getting worse, you may want to stop training now\n",
      "loss after epoch 021 is 0.560747\n",
      "loss after epoch 022 is 0.562420\n",
      "loss after epoch 023 is 0.563011\n",
      "loss after epoch 024 is 0.561312\n",
      "loss after epoch 025 is 0.563252\n",
      "loss after epoch 026 is 0.565546\n",
      "loss after epoch 027 is 0.561302\n",
      "loss after epoch 028 is 0.561511\n",
      "loss after epoch 029 is 0.561554\n",
      "loss after epoch 030 is 0.561089\n",
      "test loss is after epoch 030 is 135.289124\n",
      "test accuracy is getting worse, you may want to stop training now\n",
      "loss after epoch 031 is 0.561298\n",
      "loss after epoch 032 is 0.560937\n",
      "loss after epoch 033 is 0.560962\n",
      "loss after epoch 034 is 0.560888\n",
      "loss after epoch 035 is 0.560741\n",
      "loss after epoch 036 is 0.560531\n",
      "loss after epoch 037 is 0.561702\n",
      "loss after epoch 038 is 0.562476\n",
      "loss after epoch 039 is 0.561710\n",
      "loss after epoch 040 is 0.560657\n",
      "test loss is after epoch 040 is 145.807602\n",
      "test accuracy is getting worse, you may want to stop training now\n",
      "loss after epoch 041 is 0.560318\n",
      "loss after epoch 042 is 0.560398\n",
      "loss after epoch 043 is 0.560709\n",
      "loss after epoch 044 is 0.560656\n",
      "loss after epoch 045 is 0.561009\n",
      "loss after epoch 046 is 0.560802\n",
      "loss after epoch 047 is 0.560035\n",
      "loss after epoch 048 is 0.560046\n",
      "loss after epoch 049 is 0.560408\n",
      "loss after epoch 050 is 0.559956\n",
      "test loss is after epoch 050 is 159.754974\n",
      "test accuracy is getting worse, you may want to stop training now\n",
      "loss after epoch 051 is 0.560408\n",
      "loss after epoch 052 is 0.560227\n",
      "loss after epoch 053 is 0.560385\n",
      "loss after epoch 054 is 0.560403\n",
      "loss after epoch 055 is 0.560655\n",
      "loss after epoch 056 is 0.561279\n",
      "loss after epoch 057 is 0.561451\n",
      "loss after epoch 058 is 0.560794\n",
      "loss after epoch 059 is 0.562564\n",
      "loss after epoch 060 is 0.561262\n",
      "test loss is after epoch 060 is 195.212997\n",
      "test accuracy is getting worse, you may want to stop training now\n",
      "loss after epoch 061 is 0.560261\n",
      "loss after epoch 062 is 0.560466\n",
      "loss after epoch 063 is 0.560386\n",
      "loss after epoch 064 is 0.560296\n",
      "loss after epoch 065 is 0.560657\n",
      "loss after epoch 066 is 0.560208\n",
      "loss after epoch 067 is 0.560459\n",
      "loss after epoch 068 is 0.560557\n",
      "loss after epoch 069 is 0.560168\n",
      "loss after epoch 070 is 0.560338\n",
      "test loss is after epoch 070 is 220.247284\n",
      "test accuracy is getting worse, you may want to stop training now\n",
      "loss after epoch 071 is 0.560067\n",
      "loss after epoch 072 is 0.559887\n",
      "loss after epoch 073 is 0.560248\n",
      "loss after epoch 074 is 0.560560\n",
      "loss after epoch 075 is 0.560500\n",
      "loss after epoch 076 is 0.560078\n",
      "loss after epoch 077 is 0.559909\n",
      "loss after epoch 078 is 0.559993\n",
      "loss after epoch 079 is 0.560522\n",
      "loss after epoch 080 is 0.560919\n",
      "test loss is after epoch 080 is 263.210602\n",
      "test accuracy is getting worse, you may want to stop training now\n",
      "loss after epoch 081 is 0.560826\n",
      "loss after epoch 082 is 0.560446\n",
      "loss after epoch 083 is 0.560911\n",
      "loss after epoch 084 is 0.560432\n",
      "loss after epoch 085 is 0.562232\n",
      "loss after epoch 086 is 0.560928\n",
      "loss after epoch 087 is 0.560235\n",
      "loss after epoch 088 is 0.560883\n",
      "loss after epoch 089 is 0.561625\n",
      "loss after epoch 090 is 0.560645\n",
      "test loss is after epoch 090 is 417.211304\n",
      "test accuracy is getting worse, you may want to stop training now\n",
      "loss after epoch 091 is 0.560217\n",
      "loss after epoch 092 is 0.560067\n",
      "loss after epoch 093 is 0.560248\n",
      "loss after epoch 094 is 0.559987\n",
      "loss after epoch 095 is 0.560258\n",
      "loss after epoch 096 is 0.559909\n",
      "loss after epoch 097 is 0.560331\n",
      "loss after epoch 098 is 0.560623\n",
      "loss after epoch 099 is 0.560907\n",
      "loss after epoch 100 is 0.560636\n",
      "test loss is after epoch 100 is 439.210815\n",
      "test accuracy is getting worse, you may want to stop training now\n"
     ]
    }
   ],
   "source": [
    "best_test_score = 100\n",
    "is_saved = False\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        text_batch, label_batch = batch\n",
    "        pred = model(text_batch)\n",
    "        probs = F.softmax(pred, dim=1)\n",
    "        loss = criterion(probs, label_batch)\n",
    "        running_loss += loss / (len(train_loader))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f'loss after epoch {epoch:03} is {running_loss:3f}')\n",
    "\n",
    "    if epoch % 10 == 0 and epoch > 0:\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0\n",
    "            for i, batch in enumerate(test_loader):\n",
    "                    model.zero_grad()\n",
    "                    text_batch, label_batch = batch\n",
    "                    pred = model(text_batch)\n",
    "                    probs = F.softmax(pred, dim=1)\n",
    "                    loss = criterion(pred, label_batch)\n",
    "                    running_loss += loss / (len(test_loader))\n",
    "            print(f'test loss is after epoch {epoch:03} is {running_loss:3f}')\n",
    "            if ((running_loss < best_test_score) or (not is_saved)):\n",
    "                 best_test_score = running_loss\n",
    "                 save_dict = {}\n",
    "                 save_dict['state_dict'] = model.state_dict()\n",
    "                 torch.save(save_dict, save_path)\n",
    "                 is_saved = True\n",
    "            else:\n",
    "                 print('test accuracy is getting worse, you may want to stop training now')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load best checkpoint\n",
    "\n",
    "Now lets reload our best checkpoint to evalaute our best model with the sklearn classification report that we used in Week 6. This way we can get a direct comparison between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This cell will give you an error if the state_dict dictionary hasn't been saved, \n",
    "#i.e. if you haven't run at least 10 epochs above\n",
    "checkpoint = torch.load(save_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load test set dataframes\n",
    "\n",
    "Here we need to load the test set as a pandas dataframe to get our variables `y_test` and `class_names` needed for the classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/mb_processed_test.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ROG\\Documents\\GitHub\\NLP_TianQiu\\Week-8-Word-vectors\\classification-with-word-vectors.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-8-Word-vectors/classification-with-word-vectors.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_set_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m../data/mb_processed_test.tsv\u001b[39;49m\u001b[39m'\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-8-Word-vectors/classification-with-word-vectors.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y_test \u001b[39m=\u001b[39m test_set_df[\u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_numpy()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/NLP_TianQiu/Week-8-Word-vectors/classification-with-word-vectors.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m class_names_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../data/mb_class_labels.tsv\u001b[39m\u001b[39m'\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/mb_processed_test.tsv'"
     ]
    }
   ],
   "source": [
    "test_set_df = pd.read_csv('../data/mb_processed_test.tsv', sep='\\t')\n",
    "y_test = test_set_df['1'].to_numpy()\n",
    "\n",
    "class_names_df = pd.read_csv('../data/mb_class_labels.tsv', sep='\\t')\n",
    "class_names = list(class_names_df['class_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get classification report\n",
    "\n",
    "This code will iterate over the test set one last time. This time getting predictions for each data sample and using the [sci-kit learn classificiaton report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to give us a break down of the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    for i, batch in enumerate(test_loader):\n",
    "            model.zero_grad()\n",
    "            text_batch, label_batch = batch\n",
    "            pred = model(text_batch)\n",
    "            probs = F.softmax(pred, dim=1)\n",
    "            prob_dist = Categorical(probs)\n",
    "            pred_classes = prob_dist.sample()\n",
    "            preds += pred_classes.numpy().tolist()\n",
    "    y_pred = np.array(preds)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=class_names)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus tasks\n",
    "\n",
    "- **Task A:** Try changing some of [the hyperparameters](#define-hyperparameters), like `max_tokens`, `hidden_dim`, `learning_rate` or `batch_size` and retraining the network. Has that improved the classification accuracy?\n",
    "- **Task B:** Try adding more layers to the network in [the cell that defines the classifier](#define-the-classifier-network). Does that improve the classification accuracy?\n",
    "- **Task C:** Try changing the preprocessing of the dataset to classify all 16 personality types in `data-util/preprocess_myersbriggs.py` and running the code in [the data pre-processing cell](#optional-pre-process-data). How does the classification accuracy look now?\n",
    "- **Task D:** Adapt this code to use a different dataset. You may need to write your own preprocessing script for this. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44a0508e6db17bfb5aee65ed6759df003938501ba24285fe2dbd7c31574fa72a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
