{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a WordVec-RNN text generator\n",
    "\n",
    "In this notebook we will load and test our Word-RNN text generator. This notebook has similiarities to the `word-vector-rnn-text-gen-testing.ipynb` notebook, but this time there is no training loop, we are just loading in a model and generating from it. \n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, there are different implementations for storing and processing data on different kinds of computer hardware. By default, all computers will work by training and running neural networks on the Central Processing Unit (CPU), which we can specify with `'cpu'`. \n",
    "\n",
    "If you have an NVIDIA Graphics Processing Unit (GPU) (and you have installed CUDA correctly and the correct version of PyTorch), then you can use the flag `'gpu'` which will make training your neural networks **much faster**. Most of you won't have powerful NVIDIA GPU's in yor laptops however. Don't worry if you don't, the notebooks we are using in this class will be designed to work on laptop CPU's. \n",
    "\n",
    "If you have an M1 or M2 processor on a Mac then you can use the device `'mps'` which will run on Apples accelerated Metal Performance Shaders (MPS) for potentially faster and more powerful training (though sometimes running on CPU can be faster). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load word vectors\n",
    "\n",
    "Here we will load in our word vectors. By default we are going to download only the top30k words from GloVe, this is for speed and efficiency. You can come back to this later and try loading in some other word vectors.\n",
    "\n",
    "A torchtext Vectors class has two dictionaries in it `stoi` (string to index) and `itos` (index to string). These are equivalent to the dictionaries we made in the previous notebook `word_to_ix` and `ix_to_word`. For consistency we are going to assign variables with the same name to make this and the Week 7 `word-rnn-training.ipynb` notebooks easier to compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line and use instead of the following line to use the full GloVe dictionary\n",
    "# word_vectors = vocab.GloVe(name=\"6B\",dim=100) \n",
    "word_vectors = vocab.Vectors(name = '../data/glove.6B.100d.top30k.txt')\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "wordvec_embeddings = nn.Embedding.from_pretrained(word_vectors.vectors)\n",
    "embedding_dim = wordvec_embeddings.weight.shape[1]\n",
    "\n",
    "# Get dictionaries from word_vectors class and \n",
    "# rename to be consistent with previous notebooks\n",
    "word_to_ix = word_vectors.stoi\n",
    "ix_to_word = word_vectors.itos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyperparameters\n",
    "\n",
    "This is where we specify our *hyperparameters*. We have less hyperparameters this time as we are dont need any of the training parameters. The `hidden_size` and `num_layers` parameters need to be the same as was set when the model was trained in the other notebook.\n",
    "\n",
    "The temperature parameter can be used to control how random or conservative our precited characters will be. If we have a low temeprature (below 1) we will more often than not pick the most likely character. If the temperature is higher (than 1) our generated sequences will be more random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512   # size of hidden state\n",
    "num_layers = 3      # number of layers in LSTM layer stack\n",
    "gen_seq_len = 100   # length of LSTM sequence\n",
    "temperature = 1     # how random do we want our predictions to be\n",
    "load_path = \"wordvec_rnn_model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the network \n",
    "\n",
    "Here we define our network the same. This code must be the same as the code used in the training notebook where we saved the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, input, hidden_state):\n",
    "        output, hidden_state = self.rnn(input, hidden_state)\n",
    "        output = self.decoder(output)\n",
    "        output = self.tanh(output)\n",
    "        return output, (hidden_state[0].detach(), hidden_state[1].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up network and optimiser\n",
    "\n",
    "Here we will create an instantiation of our network `rnn`. We also need to define out loss function `loss_fn` and our `optimiser`, which is used to make make changes to the neural network weights in training. We have to make our data variable a PyTorch `tensor`. This is data type that we have to use with PyTorch so that our neural networks can read and process the data correctly. [PyTorch tensors](https://pytorch.org/docs/stable/tensors.html) have been designed to work in almost exactly the same way as [numpy arrays](https://numpy.org/doc/stable/reference/generated/numpy.array.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(load_path)\n",
    "\n",
    "# Calculate vocab size\n",
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "# Instantiate RNN\n",
    "rnn = RNN(embedding_dim, embedding_dim, hidden_size, num_layers).to(device)\n",
    "\n",
    "# Load model weights from checkpoint file \n",
    "rnn.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a random sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hidden_state = None\n",
    "    \n",
    "    random_word = random.choice(list(ix_to_word))\n",
    "    #Pick a random starting word\n",
    "    random_start = np.array(word_vectors[random_word])\n",
    "    \n",
    "    # Convert to PyTorch Tensor\n",
    "    input = torch.tensor(random_start)\n",
    "    \n",
    "    # Change dimensionality of tensor for PyTorch compatibility\n",
    "    # For more info on this function see: https://stackoverflow.com/questions/57237352/what-does-unsqueeze-do-in-pytorch\n",
    "    input = input.unsqueeze(0)\n",
    "    print(input.shape)\n",
    "    # Iterate over our sequence length\n",
    "    for i in range(gen_seq_len):\n",
    "        # Forward pass\n",
    "            output, hidden_state = rnn(input, hidden_state)\n",
    "\n",
    "            # Comput distances to all words\n",
    "            dists = torch.norm(word_vectors.vectors - output[0], dim=1) \n",
    "            # Use softmax to convert to probabilities\n",
    "            probs = F.softmax(1 - dists, dim=0)\n",
    "            # Multiply probabilities by mask to only sample words from dataset\n",
    "            probs = probs\n",
    "            # Covert probabilities to probability distribution\n",
    "            prob_dist = Categorical(probs)\n",
    "            # Sample from probability distribution\n",
    "            word_index  = prob_dist.sample()\n",
    "\n",
    "            # Get the next word and print\n",
    "            next_word = ix_to_word[word_index]\n",
    "            print(next_word, end=' ')\n",
    "            \n",
    "            # The word vector for the next word is the next input\n",
    "            input = word_vectors[next_word].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map string to indexes\n",
    "\n",
    "Lets write a function where we can manually create our own starting sequence. We will take a string and use our `word_to_ix` dictionary to get the mapped numerical values. It is important to remember that **only the words in the original dataset** will be able to be **mapped into the index values for the model**. Try printing our `word_to_ix` to see all the avaiable words. Any words not in the original data will unfortunately be skipped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_str_to_wordvec(input_str):\n",
    "    wordvec_seq = []\n",
    "    tokens = tokenizer(input_str) \n",
    "    for word in tokens:\n",
    "        ix = word_to_ix.get(word, None)\n",
    "        if ix is not None:\n",
    "            wordvec_seq.append(word_vectors[word])\n",
    "        else:\n",
    "            print(f'The char {word} is not in the dictionary')\n",
    "    # Convert list of tensors to one tensor\n",
    "    return torch.stack(wordvec_seq).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define new starting string\n",
    "\n",
    "Now lets create our index list and convert it to a numpy array then pytorch tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = 'Row, row, row, your'\n",
    "wordvec_seq = map_str_to_wordvec(input_str)\n",
    "print(f'Our sequence of word_vecs is: {wordvec_seq.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate from randomly created starting sequence\n",
    "\n",
    "Now lets have a go at generating from our own sequence. We need to have two loops here. The first passes each character into the model to update the **hidden state**, here we are not doing anything with the models predictions, just *conditioning* the model on our sequence. Once the model is conditioned on the sequence then we can start to make new generations from it in the second loop.\n",
    "\n",
    "How do these predictions compare to the random generations? What happens when you put in a starting sequence that is very different to the original data? Try [changing the temperature parameter](#set-hyperparameters) to see how the effects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hidden_state = None\n",
    "\n",
    "    print(input_str, end=' ')\n",
    "    for i in range(wordvec_seq.shape[0]):\n",
    "                \n",
    "        # Convert to PyTorch Tensor\n",
    "        input = wordvec_seq[i,:]\n",
    "\n",
    "        # Reshape tensor\n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        # Condition the model on starting sequence\n",
    "        output, hidden_state = rnn(input, hidden_state)\n",
    "        \n",
    "    input = output\n",
    "\n",
    "    # Iterate over our sequence length\n",
    "    for i in range(gen_seq_len):\n",
    "        # Forward pass\n",
    "        output, hidden_state = rnn(input, hidden_state)\n",
    "        \n",
    "        # Construct categorical distribution and sample a word\n",
    "        output = F.softmax(torch.squeeze(output), dim=0)\n",
    "        dist = Categorical(output / temperature)\n",
    "        index = dist.sample()\n",
    "        \n",
    "        # Print the sampled word\n",
    "        print(ix_to_word[index.item()], end=' ')\n",
    "        \n",
    "        # Next input is current output\n",
    "        input[0][0] = index.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try experimenting with the `temperature` parameter. How does that impact the generated results?\n",
    "\n",
    "This code does not mask for words in the original dataset. Therefore words not in the training data can be generated by this model. Can you see a difference in the vocabularly used compared the to Word RNN model from Week 7? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
