{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a text generator with WordVec-RNN \n",
    "\n",
    "In this notebook we will look at how to train a Recurrent Neural Network (RNN) to model sequence of words vectors (WordVec-RNN). This is somewhat similiar to the notebooks modelling sequences of characters and words, but there are some important differences. The size to the input of the model is determined by the embedding dimension of the word vectors (which is a fixed size), not the number of words or characters in the dataset. This means that the RNN can be fed in words that are not in the training dataset, and can also be configured to predict words it has not seen (though this current implementation [does limit the generation only to words in the training set](#masking-for-data-only-in-the-dataset), we can turn that off later).\n",
    "\n",
    "To do this we will use the library [PyTorch](https://pytorch.org/), a library for building and training neural networks in Python. This is the first time we are using this library and looking at how to build and train neural networks, but it won't be the last! We will be spending a lot of time over the next couple of terms looking at code that looks a lot like this. It may look quite unfamiliar at first, but over time you will get used to working with and altering this kind of code. \n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, there are different implementations for storing and processing data on different kinds of computer hardware. By default, all computers will work by training and running neural networks on the Central Processing Unit (CPU), which we can specify with `'cpu'`. \n",
    "\n",
    "If you have an NVIDIA Graphics Processing Unit (GPU) (and you have installed CUDA correctly and the correct version of PyTorch), then you can use the flag `'gpu'` which will make training your neural networks **much faster**. Most of you won't have powerful NVIDIA GPU's in yor laptops however. Don't worry if you don't, the notebooks we are using in this class will be designed to work on laptop CPU's. \n",
    "\n",
    "If you have an M1 or M2 processor on a Mac then you can use the device `'mps'` which will run on Apples accelerated Metal Performance Shaders (MPS) for potentially faster and more powerful training (though sometimes running on CPU can be faster). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyperparameters\n",
    "\n",
    "This is where we specify our *hyperparameters*. This is where we set the parameters that determine the size of our neural network (`num_layers`,`hidden_size` and `batch_size`), how long we train the network for (`num_steps` and `step_len`) and how aggressively we train the network (the learning rate `lr`).  \n",
    "\n",
    "`load_chk` is a boolean that determines whether we start training from the weights of an already trained model or start from scratch. If this is true, then the path to a model file should be specified in `load_path`. If you change the dataset (and have a different vocabulary size) or make changes to any other parts of the model between saving and loading a model then this will not work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512   # size of hidden state\n",
    "batch_size = 100    # length of LSTM sequence\n",
    "gen_seq_len = 50    # size of generated text sequence\n",
    "step_len = 200      # number of training samples in each stem\n",
    "num_layers = 3      # number of layers in LSTM layer stack\n",
    "lr = 0.002          # learning rate\n",
    "num_steps = 1000     # max number of training steps\n",
    "load_chk = False    # load in pre-trained checkpoint for training\n",
    "save_path = \"wordvec_rnn_model.pt\"\n",
    "# load_path = \"wordvec_rnn_model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load multiple text files\n",
    "\n",
    "This function should now look familiar. We will use it to load in our Nursery Rhymes dataset. As we are loading in all of the files into one string variable we need a way to determine where one rhyme ends and another begins. We will do this by adding in the word `EOF` (End of File) which can will the model to represent where one rhyme ends and another begins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_text_files_in_folder(path, max_files = 10000):\n",
    "    corpus = ''\n",
    "    # Find all files in the folder or subfolders\n",
    "    for root, _, files in os.walk(path):\n",
    "        for i, file in enumerate(files):\n",
    "            # If the file is a text file\n",
    "            if file.endswith(\".txt\") and i <= max_files:\n",
    "                # Open the file and add the text to the corpus\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                    # Add text from file\n",
    "                    corpus += text\n",
    "                    # Add 'End of File' between documents\n",
    "                    corpus += '\\n EOF \\n'\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load our data\n",
    "\n",
    "Now lets load in our data. We are going to use a dataset of pokemon names to start off with. We are going to use the [Python set data structure](https://www.w3schools.com/python/python_sets.asp) to find all of the unique words in our text. We will first use the `split()` function to split our corpus into words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/nursery-rhymes\"\n",
    "corpus = load_all_text_files_in_folder(data_path)\n",
    "words = sorted(list(set(corpus.split())))\n",
    "data_size, vocab_size = len(corpus.split()), len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load word vectors\n",
    "\n",
    "Here we will load in our word vectors. By default we are going to download only the top30k words from GloVe, this is for speed and efficiency. You can come back to this later and try loading in some other word vectors.\n",
    "\n",
    "A torchtext Vectors class has two dictionaries in it `stoi` (string to index) and `itos` (index to string). These are equivalent to the dictionaries we made in the previous notebook `word_to_ix` and `ix_to_word`. For consistency we are going to assign variables with the same name to make this and the Week 7 `word-rnn-training.ipynb` notebooks easier to compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line and use instead of the following line to use the full GloVe dictionary\n",
    "# word_vectors = vocab.GloVe(name=\"6B\",dim=100) \n",
    "word_vectors = vocab.Vectors(name = '../data/glove.6B.100d.top30k.txt')\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "wordvec_embeddings = nn.Embedding.from_pretrained(word_vectors.vectors)\n",
    "embedding_dim = wordvec_embeddings.weight.shape[1]\n",
    "\n",
    "# Get dictionaries from word_vectors class and \n",
    "# rename to be consistent with previous notebooks\n",
    "word_to_ix = word_vectors.stoi\n",
    "ix_to_word = word_vectors.itos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get our list of tokens form our text string using the torchtext `tokenizer` class. We will remove tokens that we do not have word vectors for. We will then create our `data` variable by getting the word vector for each token in our dataset in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(corpus) \n",
    "print(f'There raw data cosists of {len(tokens)} tokens.')\n",
    "tokens = [token for token in tokens if token in word_to_ix]\n",
    "print(f'There processed data cosists of {len(tokens)} tokens (that exist in the word vector vocab)')\n",
    "data = word_vectors.get_vecs_by_tokens(tokens, lower_case_backup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking for data only in the dataset\n",
    "\n",
    "this next bit of code is going to be useful later. Essentially what we want to do is figure out all of our unique words, get the indexes for them in our `word_vectors` matrix. We will use this information to make a variable called `mask_array` which is a list the length of the number of words in our `word_vectors` matrix. At the corresponding index of where we have words in our dataset, the value will be `1`, for words not in our dataset, the value will be `0`. We can then use this to ensure that when we sample words we only sample words in our dataset. \n",
    "\n",
    "This is not essential but it will help to keep the generations from our model bounded to the vocabularly of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vocab = sorted(list(set(tokens)))\n",
    "print(data_vocab)\n",
    "data_indexes = [word_to_ix[word] for word in data_vocab]\n",
    "print(data_indexes)\n",
    "mask_array = np.zeros(len(word_to_ix), dtype=int)\n",
    "mask_array[data_indexes] = 1\n",
    "print(mask_array)\n",
    "mask_array = torch.tensor(mask_array, dtype=torch.int64, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the network \n",
    "\n",
    "This is where we define our neural network. We define a neural network as a `class`. Classes can have **functions** and ****variables** that it owns. Classes are a fundamental part of [object-orientated programming in Python](https://www.w3schools.com/python/python_classes.asp) (and many other programming languages). In this case we are building our class `RNN` by [inheriting from the base class](https://www.w3schools.com/python/python_inheritance.asp) for a neural network in PyTorch called `nn.Module`. \n",
    "\n",
    "We need to define two functions for a PyTorch neural network class. The `__init__` function gets called when we create the class, here we create and set the variables that our network needs (such as the all of layers and other things we may need to keep track of). In this function the first thing we need to call is the `super` function that will call the `__init__` function of the base class we are inheriting from. \n",
    "\n",
    "The other function we need to define for a PyTorch neural networks is the function `forward` function. This defines what happens when we do a forward pass with our network (taking data as an input and giving somethign else as an output). Because this is a recurrent neural network, our network needs to take as input both the data and the hidden state (the previous iteration) of the model. This function also outputs the hidden state so that we can pass it back into the function at a later iteration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, input_batch, hidden_state):\n",
    "        output, hidden_state = self.rnn(input_batch, hidden_state)\n",
    "        output = self.decoder(output)\n",
    "        output = self.tanh(output)\n",
    "        return output, (hidden_state[0].detach(), hidden_state[1].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up network and optimiser\n",
    "\n",
    "Here we will create an instantiation of our network `rnn`. We also need to define out loss function `loss_fn` and our `optimiser`, which is used to make make changes to the neural network weights in training. We have to make our data variable a PyTorch `tensor`. This is data type that we have to use with PyTorch so that our neural networks can read and process the data correctly. [PyTorch tensors](https://pytorch.org/docs/stable/tensors.html) have been designed to work in almost exactly the same way as [numpy arrays](https://numpy.org/doc/stable/reference/generated/numpy.array.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of indexes that can be valid starting points for training\n",
    "index_list = list(range(0, len(data) - step_len - 1))\n",
    "\n",
    "# Conver data to torch tensor\n",
    "data = torch.tensor(data).to(device)\n",
    "data = torch.unsqueeze(data, dim=1)\n",
    "\n",
    "# Create RNN class\n",
    "rnn = RNN(embedding_dim, embedding_dim, hidden_size, num_layers).to(device)\n",
    "\n",
    "# Define loss function and optimiser\n",
    "loss_fn = nn.CosineSimilarity()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n",
    "if load_chk:\n",
    "    checkpoint = torch.load(load_path)\n",
    "    rnn.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample data randomly\n",
    "This function will allow us to do random sampling of the dataset in training. When we train neural networks we almost always train on **random batches of data**. In training we process lots of data samples at the same time (with lots of copies of the neural network), and then average our loss over all of the data samples and update the weights accordingly. This helps with the *regularisation* of the network, and makes training much more stable. \n",
    "\n",
    "The number of data samples we have in each mini-batch is defined by the `batch_size`, generally speaking the more batches you can use the better (though there are exceptions to this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_batch_indicies(index_list, batch_size):\n",
    "    # Get a batch of indicies to sample our data from\n",
    "    input_batch_indicies = torch.tensor(np.array(random.sample(index_list, batch_size)))\n",
    "    # Offset indicies for target batch by one\n",
    "    target_batch_indicies = input_batch_indicies + 1\n",
    "    return input_batch_indicies, target_batch_indicies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the network\n",
    "\n",
    "Here we have all the code we need for our **training loop**. Here we are looping through our number of training steps (defined in `num_steps`). Each step we will sample a random section of a dataset that will cycle for `step_len` training iterations. In some code bases, you will see code that cycles through *epochs* (complete cycles of the dataset). We aren't doing that here as the training time can vary drastically based on how much data is in your dataset. \n",
    "\n",
    "After each iteration the weights of the model will be saved to the file `wordvec_rnn_model.pt`. Sometimes people will save different versions of the file after a set number of step (i.e. `wordvec_rnn_model_50.pt` or `wordvec_rnn_model_100.pt`), but this will fill up your drive very quickly! For now we will just keep overwriting the same file after each iteration. \n",
    "\n",
    "If you are happy with the outputs or need to stop the code running for whatever reason, you can just kill the cell and your progress will be saved. This can be loaded into the notebook `wordvec-rnn-text-gen-test.ipynb` to be used for code that just performs generation. \n",
    "\n",
    "The most important parts of any training code are the **forward pass** (where we process our data with our neural network), calculating the **loss function** where evaluate how well our model has performed against the real value of the data. Then we have to **update the weights of the neural network**. This is done by calling `loss.backward()` followed by `optimizer.step()`.\n",
    "\n",
    "After each iteration of the code we generate a new sequence with the network so we can see how the network is improving during training. This is done without gradient tracking `with torch.no_grad():` (gradient tracking is what is used for training and calculating how much to adjust the weights of our network by at each step). \n",
    "\n",
    "This will probably all look quite complicated and hard to understand first time around. **That is ok!** Over time as you see and work with more and more code that looks like this you will start getting used to it and feel more confident in adapting, changing and writing this kind of code yourself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the number of steps defined earlier\n",
    "for step in range(1, num_steps):\n",
    "    \n",
    "    running_loss = 0\n",
    "    hidden_state = None\n",
    "    rnn.zero_grad()\n",
    "    train_batch_indicies, target_batch_indicies = get_training_batch_indicies(index_list, batch_size)\n",
    "    # Cycle through for a set number of consecutive iterations in the data\n",
    "    for i in range(step_len):\n",
    "        input_batch = data[train_batch_indicies].squeeze()\n",
    "        target_batch = data[target_batch_indicies].squeeze()\n",
    "    \n",
    "        # Forward pass\n",
    "        # The following code is the same as calling rnn.forward(input_batch, hidden_state)\n",
    "        output, hidden_state = rnn(input_batch, hidden_state)\n",
    "        \n",
    "        # Compute loss (we take 1 minus the loss as it is measure similiarity not distance)\n",
    "        loss = 1 - loss_fn(output, target_batch).mean() \n",
    "        running_loss += loss.item() / step_len\n",
    "        # print(f'loss {loss}, running loss {running_loss}')\n",
    "        \n",
    "        # Update weights of neural network\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Increment batch indicies by 1\n",
    "        train_batch_indicies = train_batch_indicies + 1\n",
    "        target_batch_indicies = target_batch_indicies + 1\n",
    "        \n",
    "\n",
    "        \n",
    "    # Print loss\n",
    "    print('\\n'+'-'*75)\n",
    "    print(f\"\\nStep: {step} Loss: {running_loss}\")\n",
    "\n",
    "    # Create a dictionary for saving the model and data mappings\n",
    "    save_dict = {}\n",
    "    # Add the model weight parameters as a dictionary to our save_dict\n",
    "    save_dict['state_dict'] = rnn.state_dict()\n",
    "    # Add the idx_to_char and char_to_idx dicts to our save_dict\n",
    "    save_dict['ix_to_word'] = ix_to_word\n",
    "    save_dict['word_to_ix'] = word_to_ix\n",
    "    # Save the dictionary to a file\n",
    "    torch.save(save_dict, save_path)\n",
    "\n",
    "    # Now lets generate a random generated text sample to print out,\n",
    "    # we will do this without gradient tracking as we are not training\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Take a random index and reset the hidden state of the model\n",
    "        rand_index = np.random.randint(data_size-1)\n",
    "        input = data[rand_index : rand_index+1].squeeze().unsqueeze(0)\n",
    "        hidden_state = None\n",
    "        \n",
    "        # Iterate over our sequence length\n",
    "        for i in range(gen_seq_len):\n",
    "            \n",
    "            # Forward pass\n",
    "            output, hidden_state = rnn(input, hidden_state)\n",
    "\n",
    "            # Comput distances to all words\n",
    "            dists = torch.norm(word_vectors.vectors - output[0], dim=1) \n",
    "            # Use softmax to convert to probabilities\n",
    "            probs = F.softmax(1 - dists, dim=0)\n",
    "            # Multiply probabilities by mask to only sample words from dataset\n",
    "            probs = probs * mask_array\n",
    "            # Covert probabilities to probability distribution\n",
    "            prob_dist = Categorical(probs)\n",
    "            # Sample from probability distribution\n",
    "            word_index  = prob_dist.sample()\n",
    "\n",
    "            # Get the next word and print\n",
    "            next_word = ix_to_word[word_index]\n",
    "            print(next_word, end=' ')\n",
    "            \n",
    "            # The word vector for the next word is the next input\n",
    "            input = word_vectors[next_word].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks \n",
    "\n",
    "First do these tasks in order before moving onto the bonus tasks:\n",
    "\n",
    "**Task 1:** Run the code cells and train a model on the Nursery Rhymes names. Is it possble after training to generate plausible looking nursery rhymes. If the model training has finished and you are not happy with the results you can set `load_chk` to `True` in [the cell that defines the hyperparameters](#set-hyperparameters) and load in a model from the variable `load_path`.\n",
    "\n",
    "**Task 2:** Load in a trained model into the `word-vector-rnn-text-gen-testing.ipynb` notebook. Play around with the `temperature` parameter and turn off the dataset mask to see how that effects the results.\n",
    "\n",
    "**Task 3:** Compare the code in this notebook to the Word-RNN training (`Week7-Generating-text-with-neural-networks/word-rnn-training.ipynb`). How does the code differ? What elements are the same between the notebooks.\n",
    "\n",
    "**Task 4:** Adapt this code to load in another dataset. Have a look at the code in `text-generation-with-markov-chains.ipynb` from Week 5 to use functions to load in datasets in different formats. Are the results better on datasets that uses more standard Englsh than the nursery rhymes dataset?\n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "These bonus tasks can be done in any order.\n",
    "\n",
    "After each training run you may want to rename the checkpoint files from each training run so you can keep them for comparison later.\n",
    "\n",
    "**Task A:** Try [changing the word vectors used in this code](#load-word-vectors). Does using a larger set of word vectors, a different dimensionality, or [a completely different set of word vectors](https://torchtext.readthedocs.io/en/latest/vocab.html#pretrained-word-embeddings) make any significiant qualititaive changes to the output or behaviour in training?\n",
    "\n",
    "**Task B:** Try changing some of the other hyperparameters in [the cell that defines the hyperparameters](#set-hyperparameters). Such as `hidden_size` `seq_len`, `num_layers` or `lr`. Restart the kernel and run the training again. \n",
    "\n",
    "**Task C:** Try changing the optimiser used [the cell where the network and optimiser are instantiated](#setting-up-network-and-optimiser) to one of [the other optimisers available in PyTorch](https://pytorch.org/docs/stable/optim.html), such as stochastic gradient descent (SGD) or Adagrad. Restart the kernel and run the training again. \n",
    "\n",
    "**Task D:** Try changing the type of layer used [in the RNN network](#defining-the-network) from LSTM to a [vanilla RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN) or a [Gated Recurrent Unit](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU) (GRU). Restart the kernel and run the training again. \n",
    "\n",
    "**Task E:** Can you use stop words or do some other kinds of cleaning or normalisation to the dataset to improve or edit the quality of the generated results? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
