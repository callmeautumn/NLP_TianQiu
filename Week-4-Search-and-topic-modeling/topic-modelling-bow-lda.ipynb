{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Topic modelling nursery rhymes with Bag of Words features and Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "In this notebook we are going to look at how to perform topic modelling with Bag of Words as the input features. There is another notebook **very similiar** to this one, except it uses [**TF-IDF**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (Term Frequency - Inverse Document Frequency) as the features for topic modelling along with a different topic modelling algorithm. Compare the results from this notebook to the TF-IDF one and see how the code and results differ. \n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets download a library of English stop words and the semantic word database [wordnet](https://wordnet.princeton.edu/https://wordnet.princeton.edu/) that we will use for lemmatisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to define this function which gets us the [Part of Speech tag](https://en.wikipedia.org/wiki/Part-of-speech_tagging) (POS), to tell us what type of word each word in our dataset is, such as whether a word is a [noun](https://www.merriam-webster.com/dictionary/noun), a [verb](https://www.merriam-webster.com/dictionary/verb), an [adjective](https://www.merriam-webster.com/dictionary/adjective) or an [adverb](https://www.merriam-webster.com/dictionary/adverb). There are other POS tags, but these are the four we need for the NLTK lemmatiser.\n",
    "\n",
    "This will help us when we come to perform lemmatisation, as this gives us more context about each word and makes our lemmatisation algorithm more effective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function originally from: https://www.programcreek.com/python/?CodeExample=get%20wordnet%20pos\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    \n",
    "    # Return the tag, if the tag is not found return noun. \n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function goes through every text document in a folder and performs lemmatisation on the contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_documents(folder_path):\n",
    "    document_texts = []\n",
    "    document_labels = []\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                \n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                # Apply lemmatizer to each word in the nursery rhyme\n",
    "                lemmitized_text = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in text.split()])\n",
    "                document_texts.append(lemmitized_text)\n",
    "                document_labels.append(os.path.basename(file[:-4]))\n",
    "    \n",
    "    return document_texts, document_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put in the path for the nursery rhyme dataset and load in the documents:\n",
    "\n",
    "<a id='load-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../data/nursery-rhymes\"\n",
    "document_texts, document_labels = load_text_documents(folder_path)\n",
    "print(f'loaded {len(document_labels)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the first document and see it has loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The first document is {document_labels[0]}, which goes:')\n",
    "print(document_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define our stop words. We are combining generic English stop words with stop words specific to our dataset of nursery rhymes (if you adapt this code to another dataset, **make sure to modify these stop words**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = stopwords.words('english')\n",
    "nursery_rhyme_stop_words = ['chorus', 'repeat', '3x', 'verse', 'version', 'versions', 'intro', 'finale', 'lyrics']\n",
    "stop_words = english_stop_words + nursery_rhyme_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use the `CountVectorizer` class to get our bag of words features for each document:\n",
    "\n",
    "<a id='vectorizer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stop_words, ngram_range=(1,1))\n",
    "bag_of_words = vectorizer.fit_transform(document_texts)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(f'Our bag of words is a matrix of the shape and size {bag_of_words.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at our bag of words features matrix (aka a table) for all documents as a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_df = pd.DataFrame(bag_of_words.toarray(), columns=vocab, index=document_labels)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the bag of words features for the first nusery rhyme. We will remove all of the words with zero counts to make it easier to make sense of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_row_df = bow_df.iloc[0]\n",
    "single_row_df = single_row_df.replace(0.0,None)\n",
    "single_row_df = single_row_df.dropna()\n",
    "single_row_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define the number of topics we are using. Come back to this follwing cell later on to change the number of topics you are using:\n",
    "\n",
    "<a id='num-topics'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 16\n",
    "pd.options.display.max_columns=num_topics #Make sure we display them all\n",
    "labels = ['topic{}'.format(i) for i in range(num_topics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define our Latent Dirichlet Allocation (LDA) algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=num_topics,random_state=123, learning_method='batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets fit our LDA model to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics = lda.fit_transform(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see some of the weightings between our topics and our words.\n",
    "\n",
    "(Note that lda.components_ is a 2d array of elements, each of which is higher if the association between the given topic and word is stronger, and lower if the association is weaker. This is not a normalised probability distribution, so the elements within a topic won't sum to 1. Note also that lda.components_ has topics in the rows and words in the columns, so we can use .T to get the \"transposed\" version in which rows and columns are swapped.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_weights = pd.DataFrame(lda.components_.T, index=vocab, columns=labels)\n",
    "topic_weights.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the most relevent words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_terms = 20\n",
    "for i in range(num_topics):\n",
    "    print(\"___topic \" + str(i) + \"___\")\n",
    "    topicName = \"topic\" + str(i)\n",
    "    weightedlist = topic_weights.get(topicName).sort_values()[-num_terms:]\n",
    "    print(weightedlist.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the association between our documents (individual nursery rhymes or other data samples) and our topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic_vectors_df = pd.DataFrame(lda_topics, index=document_labels, columns=labels)\n",
    "lda_topic_vectors_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can sort by importance for a particular topic. \n",
    "\n",
    "Try changing the topic that you are sorting by and see if you can see a correspondence between the most import words in the topic with the lyrics of the nursery rhyme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic_vectors_df.sort_values(by=['topic1'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "**Task 1:** Compare this notebook to the TF-IDF + LSA topic modelling notebook. What differences do you see? Are the topics any better when using the other algorithm?\n",
    "\n",
    "**Task 2:** Change the [number of topics](#num-topics). How does that effect the topics? Is using more or less topics better?\n",
    "\n",
    "**Task 3:** Adjust the n-gram parameters [in the cell that defines the bag of words vectorizer](#vectorizer), i.e. make the range `1,2` if you want to include individual words and bi-grams, or `2,3` if you want to use bi-grams and tri-grams. How does that effect the topics?\n",
    "\n",
    "**Task 4:** Once you have done that, try loading in a different dataset and try out topic modelling on that. There is a [dataset of limericks](https://git.arts.ac.uk/tbroad/limerick-dataset), a [dataset of haikus](https://git.arts.ac.uk/tbroad/haiku-dataset), and a [dataset of EPL fan chants](https://git.arts.ac.uk/tbroad/SFW-EPL-fan-chants-dataset) (nursery rhymes for grown men) that have been created to be in the same format as the nursery rhymes dataset. Simply download them (unzip if you need to) and move the dataset folder into the folder `../data/my-data` and [edit the path](#load-data) for the new dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
