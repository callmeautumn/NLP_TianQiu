{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Topic modelling nursery rhymes with TF-IDF features and Latent Semantic Analysis (LSA)\n",
    "\n",
    "In this notebook we are going to look at how to perform topic modelling with [**TF-IDF**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (Term Frequency - Inverse Document Frequency) as the input features. There is another notebook **very similiar** to this one, except it uses Bag of Words (BoW) as the features for topic modelling and the LDA algorithm. Compare the results from this notebook to the BoW one and see how the code and results differ. \n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets download a library of English stop words and the semantic word database [wordnet](https://wordnet.princeton.edu/https://wordnet.princeton.edu/) that we will use for lemmatisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ROG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ROG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ROG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to define this function which gets us the [Part of Speech tag](https://en.wikipedia.org/wiki/Part-of-speech_tagging) (POS), to tell us what type of word each word in our dataset is, such as whether a word is a [noun](https://www.merriam-webster.com/dictionary/noun), a [verb](https://www.merriam-webster.com/dictionary/verb), an [adjective](https://www.merriam-webster.com/dictionary/adjective) or an [adverb](https://www.merriam-webster.com/dictionary/adverb). There are other POS tags, but these are the four we need for the NLTK lemmatiser.\n",
    "\n",
    " This will help us when we come to perform lemmatisation, as this gives us more context about each word and makes our lemmatisation algorithm more effective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function originally from: https://www.programcreek.com/python/?CodeExample=get%20wordnet%20pos\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function goes through every text document in a folder and performs lemmatisation on the contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_documents(folder_path):\n",
    "    document_texts = []\n",
    "    document_labels = []\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                \n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                # Apply lemmatizer to each word in the nursery rhyme\n",
    "                lemmitized_text = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in text.split()])\n",
    "                document_texts.append(lemmitized_text)\n",
    "                document_labels.append(os.path.basename(file[:-4]))\n",
    "    \n",
    "    return document_texts, document_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put in the path for the nursery rhyme dataset and load in the documents:\n",
    "\n",
    "<a id='load-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\genericpath.py:42\u001b[0m, in \u001b[0;36misdir\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(s)\n\u001b[0;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 系统找不到指定的文件。: 'c:\\\\Users\\\\ROG\\\\anaconda3\\\\envs\\\\stem\\\\share\\\\nltk_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ROG\\Documents\\GitHub\\MySTEM\\NLP-23-24\\Week-4-Search-and-topic-modeling\\topic-modelling-tfidf-lsa.ipynb 单元格 10\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# folder_path = r\"D:\\OneDrive - University of the Arts London\\NLP-23-24\\data\\test\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m folder_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../data/haikus\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m document_texts, document_labels \u001b[39m=\u001b[39m load_text_documents(folder_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloaded \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(document_labels)\u001b[39m}\u001b[39;00m\u001b[39m documents\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\ROG\\Documents\\GitHub\\MySTEM\\NLP-23-24\\Week-4-Search-and-topic-modeling\\topic-modelling-tfidf-lsa.ipynb 单元格 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Apply lemmatizer to each word in the nursery rhyme\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m lemmitized_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([lemmatizer\u001b[39m.\u001b[39mlemmatize(word, get_wordnet_pos(word)) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m text\u001b[39m.\u001b[39msplit()])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m document_texts\u001b[39m.\u001b[39mappend(lemmitized_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m document_labels\u001b[39m.\u001b[39mappend(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(file[:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m]))\n",
      "\u001b[1;32mc:\\Users\\ROG\\Documents\\GitHub\\MySTEM\\NLP-23-24\\Week-4-Search-and-topic-modeling\\topic-modelling-tfidf-lsa.ipynb 单元格 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Apply lemmatizer to each word in the nursery rhyme\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m lemmitized_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([lemmatizer\u001b[39m.\u001b[39mlemmatize(word, get_wordnet_pos(word)) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m text\u001b[39m.\u001b[39msplit()])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m document_texts\u001b[39m.\u001b[39mappend(lemmitized_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m document_labels\u001b[39m.\u001b[39mappend(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(file[:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m]))\n",
      "\u001b[1;32mc:\\Users\\ROG\\Documents\\GitHub\\MySTEM\\NLP-23-24\\Week-4-Search-and-topic-modeling\\topic-modelling-tfidf-lsa.ipynb 单元格 10\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_wordnet_pos\u001b[39m(word):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     tag \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mpos_tag([word])[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mupper()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     tag_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mJ\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39mADJ,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mN\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39mNOUN,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mV\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39mVERB,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mR\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39mADV}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ROG/Documents/GitHub/MySTEM/NLP-23-24/Week-4-Search-and-topic-modeling/topic-modelling-tfidf-lsa.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tag_dict\u001b[39m.\u001b[39mget(tag, wordnet\u001b[39m.\u001b[39mNOUN)\n",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos_tag\u001b[39m(tokens, tagset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[39m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[39m=\u001b[39m PerceptronTagger()\n\u001b[0;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         find(\u001b[39m\"\u001b[39;49m\u001b[39mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m PICKLE)\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\site-packages\\nltk\\data.py:530\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[39m# Is the path item a directory or is resource_name an absolute path?\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m path_ \u001b[39mor\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49misdir(path_):\n\u001b[0;32m    531\u001b[0m     \u001b[39mif\u001b[39;00m zipfile \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m         p \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path_, url2pathname(resource_name))\n",
      "File \u001b[1;32mc:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\genericpath.py:42\u001b[0m, in \u001b[0;36misdir\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return true if the pathname refers to an existing directory.\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(s)\n\u001b[0;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# folder_path = r\"D:\\OneDrive - University of the Arts London\\NLP-23-24\\data\\test\"\n",
    "folder_path = \"../data/haikus\"\n",
    "document_texts, document_labels = load_text_documents(folder_path)\n",
    "print(f'loaded {len(document_labels)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the first document and see it has loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first document is , which goes:\n",
      "Age how sure!\n"
     ]
    }
   ],
   "source": [
    "print(f'The first document is {document_labels[0]}, which goes:')\n",
    "print(document_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define our stop words. We are combining generic English stop words with stop words specific to our dataset of nursery rhymes (if you adapt this code to another dataset, **make sure to modify these stop words**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = stopwords.words('english')\n",
    "nursery_rhyme_stop_words = ['chorus', 'repeat', '3x', 'verse', 'version', 'versions', 'intro', 'finale', 'lyrics']\n",
    "stop_words = english_stop_words + nursery_rhyme_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use the `TfidfVectorizer` class to get our TF-IDF features for each document:\n",
    "\n",
    "<a id='vectorizer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our bag of words is a matrix of the shape and size (27081, 19256)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=(1,1))\n",
    "tf_idf = vectorizer.fit_transform(document_texts)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(f'Our bag of words is a matrix of the shape and size {tf_idf.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at our TF-IDF features matrix (aka a table) for all documents as a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuleika</th>\n",
       "      <th>ēn</th>\n",
       "      <th>ēng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-11-11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.819225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-a-yellow-band-of-light-upon-the-street</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10-my-ship-has-tasted</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37681</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-degree-heat</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.44783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gives-hope-to-the-valiant-and-promise-of-war</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gives-them-extra-time</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gives-too-late-whats-not-believed</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gives-too-soon-into-weak-hands-whats</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>giving-a-sublime</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27081 rows × 19256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               00   01       10      100  \\\n",
       "                                              0.0  0.0  0.00000  0.00000   \n",
       "1-11-11                                       0.0  0.0  0.00000  0.00000   \n",
       "1-a-yellow-band-of-light-upon-the-street      0.0  0.0  0.00000  0.00000   \n",
       "10-my-ship-has-tasted                         0.0  0.0  0.37681  0.00000   \n",
       "100-degree-heat                               0.0  0.0  0.00000  0.44783   \n",
       "...                                           ...  ...      ...      ...   \n",
       "gives-hope-to-the-valiant-and-promise-of-war  0.0  0.0  0.00000  0.00000   \n",
       "gives-them-extra-time                         0.0  0.0  0.00000  0.00000   \n",
       "gives-too-late-whats-not-believed             0.0  0.0  0.00000  0.00000   \n",
       "gives-too-soon-into-weak-hands-whats          0.0  0.0  0.00000  0.00000   \n",
       "giving-a-sublime                              0.0  0.0  0.00000  0.00000   \n",
       "\n",
       "                                                    11  ...  zucchini  \\\n",
       "                                              0.000000  ...       0.0   \n",
       "1-11-11                                       0.819225  ...       0.0   \n",
       "1-a-yellow-band-of-light-upon-the-street      0.000000  ...       0.0   \n",
       "10-my-ship-has-tasted                         0.000000  ...       0.0   \n",
       "100-degree-heat                               0.000000  ...       0.0   \n",
       "...                                                ...  ...       ...   \n",
       "gives-hope-to-the-valiant-and-promise-of-war  0.000000  ...       0.0   \n",
       "gives-them-extra-time                         0.000000  ...       0.0   \n",
       "gives-too-late-whats-not-believed             0.000000  ...       0.0   \n",
       "gives-too-soon-into-weak-hands-whats          0.000000  ...       0.0   \n",
       "giving-a-sublime                              0.000000  ...       0.0   \n",
       "\n",
       "                                              zuckerberg  zuleika   ēn  ēng  \n",
       "                                                     0.0      0.0  0.0  0.0  \n",
       "1-11-11                                              0.0      0.0  0.0  0.0  \n",
       "1-a-yellow-band-of-light-upon-the-street             0.0      0.0  0.0  0.0  \n",
       "10-my-ship-has-tasted                                0.0      0.0  0.0  0.0  \n",
       "100-degree-heat                                      0.0      0.0  0.0  0.0  \n",
       "...                                                  ...      ...  ...  ...  \n",
       "gives-hope-to-the-valiant-and-promise-of-war         0.0      0.0  0.0  0.0  \n",
       "gives-them-extra-time                                0.0      0.0  0.0  0.0  \n",
       "gives-too-late-whats-not-believed                    0.0      0.0  0.0  0.0  \n",
       "gives-too-soon-into-weak-hands-whats                 0.0      0.0  0.0  0.0  \n",
       "giving-a-sublime                                     0.0      0.0  0.0  0.0  \n",
       "\n",
       "[27081 rows x 19256 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(tf_idf.toarray(), columns=vocab, index=document_labels)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the TF-IDF features for the first nusery rhyme. We will remove all of the words with zero counts to make it easier to make sense of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age     0.730377\n",
       "sure    0.683045\n",
       "Name: , dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_row_df = tfidf_df.iloc[0]\n",
    "single_row_df = single_row_df.replace(0.0,None)\n",
    "single_row_df = single_row_df.dropna()\n",
    "single_row_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing LSA (TruncatedSVD)\n",
    "\n",
    "Subtrach the mean from each value in the matrix/dataframe/table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = tfidf_df - tfidf_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set the number of topics you want in the following cell:\n",
    "\n",
    "<a id='num-topics'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 11\n",
    "pd.options.display.max_columns=num_topics #Make sure we display them all\n",
    "labels = ['topic{}'.format(i) for i in range(num_topics)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets calculate our topics using the [TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) (aka LSA) algorithm from sci-kit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components = num_topics, n_iter = 11) #You can change n_iter: Higher numbers will take longer but may (or may not) give you better results\n",
    "svd_topic_vectors = svd.fit_transform(tfidf_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see some of the weightings between our topics and our words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>topic10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>-0.003822</td>\n",
       "      <td>-0.002775</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>-4.309417e-03</td>\n",
       "      <td>-0.004937</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.001115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overlord</th>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-8.346027e-05</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw</th>\n",
       "      <td>-0.000780</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>-0.000670</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>-6.514606e-04</td>\n",
       "      <td>-0.000274</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>-0.000904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gasworks</th>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>-0.001090</td>\n",
       "      <td>-0.000324</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>7.763137e-05</td>\n",
       "      <td>-0.000321</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corners</th>\n",
       "      <td>-0.000281</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>-1.449977e-05</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nemnan</th>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-5.003342e-06</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precise</th>\n",
       "      <td>-0.000249</td>\n",
       "      <td>-0.000243</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-3.874976e-06</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>-0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blatantly</th>\n",
       "      <td>-0.000150</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000191</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>2.648841e-05</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightly</th>\n",
       "      <td>-0.000544</td>\n",
       "      <td>-0.000523</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000371</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>3.713471e-05</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slider</th>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000101</td>\n",
       "      <td>-0.000245</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>-0.000353</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.000558</td>\n",
       "      <td>-4.745958e-04</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physically</th>\n",
       "      <td>-0.000526</td>\n",
       "      <td>-0.000494</td>\n",
       "      <td>-0.000385</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-1.258241e-04</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recur</th>\n",
       "      <td>-0.000101</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-6.021090e-07</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buoy</th>\n",
       "      <td>-0.000625</td>\n",
       "      <td>-0.000594</td>\n",
       "      <td>-0.000186</td>\n",
       "      <td>-0.000309</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>-0.000293</td>\n",
       "      <td>-4.001031e-05</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>-0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extinct</th>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>-0.001322</td>\n",
       "      <td>-0.001507</td>\n",
       "      <td>-0.000734</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>-0.000425</td>\n",
       "      <td>6.168678e-05</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000645</td>\n",
       "      <td>-0.001077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mirror</th>\n",
       "      <td>0.001038</td>\n",
       "      <td>-0.000383</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>-0.000255</td>\n",
       "      <td>-0.004858</td>\n",
       "      <td>-0.002945</td>\n",
       "      <td>-0.000218</td>\n",
       "      <td>-9.923690e-04</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>-0.003073</td>\n",
       "      <td>-0.000233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>late</th>\n",
       "      <td>0.001849</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>-0.002313</td>\n",
       "      <td>-0.001675</td>\n",
       "      <td>-0.002723</td>\n",
       "      <td>-0.001703</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>-2.720936e-03</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.005778</td>\n",
       "      <td>0.013071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labourer</th>\n",
       "      <td>-0.000312</td>\n",
       "      <td>-0.000260</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>-0.000353</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>-4.044417e-04</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bentley</th>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>-0.000693</td>\n",
       "      <td>-0.000894</td>\n",
       "      <td>-0.001108</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>-3.883007e-04</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.001386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>placement</th>\n",
       "      <td>-0.000184</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>-0.000395</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>1.484712e-04</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>-0.000288</td>\n",
       "      <td>0.000124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assist</th>\n",
       "      <td>-0.000588</td>\n",
       "      <td>-0.000589</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000207</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>8.938966e-04</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>-0.000407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              topic0    topic1    topic2    topic3    topic4    topic5  \\\n",
       "low        -0.003822 -0.002775 -0.000069 -0.001013  0.000996 -0.000060   \n",
       "overlord   -0.000099 -0.000093  0.000003 -0.000108  0.000018  0.000066   \n",
       "raw        -0.000780 -0.000540  0.000141 -0.000670  0.000058  0.000058   \n",
       "gasworks    0.000151  0.000628 -0.001090 -0.000324 -0.000173 -0.000120   \n",
       "corners    -0.000281 -0.000278 -0.000082 -0.000155 -0.000117 -0.000077   \n",
       "nemnan     -0.000120 -0.000104 -0.000018 -0.000102 -0.000006  0.000004   \n",
       "precise    -0.000249 -0.000243 -0.000073 -0.000140 -0.000031 -0.000022   \n",
       "blatantly  -0.000150 -0.000133  0.000048 -0.000191  0.000109  0.000078   \n",
       "lightly    -0.000544 -0.000523 -0.000065 -0.000371 -0.000039  0.000078   \n",
       "slider     -0.000160 -0.000101 -0.000245  0.000871 -0.000353 -0.000125   \n",
       "physically -0.000526 -0.000494 -0.000385 -0.000600 -0.000164  0.000038   \n",
       "recur      -0.000101 -0.000096 -0.000026 -0.000051 -0.000001 -0.000011   \n",
       "buoy       -0.000625 -0.000594 -0.000186 -0.000309 -0.000199 -0.000167   \n",
       "extinct     0.000389  0.002111 -0.001322 -0.001507 -0.000734 -0.000193   \n",
       "mirror      0.001038 -0.000383  0.002303 -0.000255 -0.004858 -0.002945   \n",
       "late        0.001849  0.001440 -0.002313 -0.001675 -0.002723 -0.001703   \n",
       "labourer   -0.000312 -0.000260 -0.000299  0.000786 -0.000353 -0.000178   \n",
       "bentley     0.000305  0.001199  0.001347 -0.000693 -0.000894 -0.001108   \n",
       "placement  -0.000184  0.000190  0.000492 -0.000395  0.000252 -0.000006   \n",
       "assist     -0.000588 -0.000589 -0.000168 -0.000178 -0.000077 -0.000207   \n",
       "\n",
       "              topic6        topic7    topic8    topic9   topic10  \n",
       "low         0.002346 -4.309417e-03 -0.004937  0.003925  0.001115  \n",
       "overlord    0.000024 -8.346027e-05 -0.000058 -0.000059  0.000044  \n",
       "raw        -0.000196 -6.514606e-04 -0.000274  0.001178 -0.000904  \n",
       "gasworks   -0.000271  7.763137e-05 -0.000321 -0.000055 -0.000175  \n",
       "corners    -0.000143 -1.449977e-05 -0.000113  0.000037 -0.000047  \n",
       "nemnan     -0.000010 -5.003342e-06 -0.000021  0.000001 -0.000054  \n",
       "precise    -0.000054 -3.874976e-06 -0.000111  0.000043 -0.000047  \n",
       "blatantly  -0.000036  2.648841e-05  0.000035 -0.000032 -0.000026  \n",
       "lightly    -0.000146  3.713471e-05 -0.000270 -0.000001 -0.000057  \n",
       "slider     -0.000558 -4.745958e-04  0.000480  0.000029  0.000049  \n",
       "physically -0.000086 -1.258241e-04 -0.000114 -0.000065 -0.000401  \n",
       "recur      -0.000019 -6.021090e-07 -0.000064  0.000028 -0.000007  \n",
       "buoy       -0.000293 -4.001031e-05 -0.000184  0.000114 -0.000101  \n",
       "extinct    -0.000425  6.168678e-05 -0.000056 -0.000645 -0.001077  \n",
       "mirror     -0.000218 -9.923690e-04  0.001866 -0.003073 -0.000233  \n",
       "late        0.002187 -2.720936e-03  0.002935  0.005778  0.013071  \n",
       "labourer   -0.000504 -4.044417e-04  0.000362  0.000037 -0.000021  \n",
       "bentley     0.000071 -3.883007e-04  0.000185  0.000644  0.001386  \n",
       "placement  -0.000099  1.484712e-04  0.000116 -0.000288  0.000124  \n",
       "assist      0.000564  8.938966e-04  0.000125  0.000129 -0.000407  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "topic_weights = pd.DataFrame(svd.components_.T, index=vocab, columns=labels)\n",
    "topic_weights.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the most relevent words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___topic 0___\n",
      "['wanna' 'way' 'one' 'people' 'make' 'someone' 'let' 'thing' 'everyone'\n",
      " 'time' 'ever' 'feel' 'back' 'want' 'know' 'shit' 'fuck' 'like' 'go' 'get']\n",
      "___topic 1___\n",
      "['see' 'back' 'someone' 'ever' 'really' 'say' 'never' 'thing' 'always'\n",
      " 'look' 'want' 'one' 'people' 'even' 'make' 'love' 'feel' 'know' 'like'\n",
      " 'go']\n",
      "___topic 2___\n",
      "['good' 'thing' 'best' 'everyone' 'person' 'shit' 'bad' 'much' 'say'\n",
      " 'always' 'really' 'ever' 'fuck' 'even' 'people' 'look' 'love' 'make'\n",
      " 'feel' 'like']\n",
      "___topic 3___\n",
      "['ever' 'great' 'last' 'long' 'night' 'well' 'happy' 'feel' 'best' 'thing'\n",
      " 'another' 'year' 'first' 'good' 'make' 'every' 'time' 'love' 'one' 'day']\n",
      "___topic 4___\n",
      "['think' 'well' 'thing' 'need' 'people' 'person' 'get' 'everyone' 'tell'\n",
      " 'let' 'want' 'give' 'someone' 'say' 'always' 'never' 'even' 'much' 'know'\n",
      " 'love']\n",
      "___topic 5___\n",
      "['cry' 'decision' 'way' 'happen' 'hurt' 'life' 'need' 'friend' 'sure'\n",
      " 'good' 'know' 'bad' 'someone' 'really' 'well' 'happy' 'people' 'thing'\n",
      " 'want' 'make']\n",
      "___topic 6___\n",
      "['man' 'never' 'shit' 'take' 'first' 'bad' 'always' 'need' 'really' 'best'\n",
      " 'say' 'want' 'think' 'see' 'thing' 'ever' 'people' 'know' 'time' 'one']\n",
      "___topic 7___\n",
      "['take' 'someone' 'baby' 'last' 'happy' 'waste' 'year' 'watch' 'fall'\n",
      " 'long' 'like' 'go' 'back' 'every' 'feel' 'come' 'first' 'love' 'make'\n",
      " 'time']\n",
      "___topic 8___\n",
      "['ever' 'see' 'everyone' 'damn' 'talk' 'first' 'life' 'need' 'every'\n",
      " 'think' 'give' 'really' 'say' 'shit' 'time' 'want' 'know' 'people' 'day'\n",
      " 'fuck']\n",
      "___topic 9___\n",
      "['dark' 'sun' 'moon' 'yet' 'sad' 'summer' 'light' 'tho' 'last' 'look'\n",
      " 'long' 'well' 'feel' 'first' 'get' 'though' 'still' 'time' 'know' 'even']\n",
      "___topic 10___\n",
      "['someone' 'week' 'end' 'day' 'anyone' 'new' 'year' 'let' 'good' 'baby'\n",
      " 'home' 'look' 'see' 'never' 'everyone' 'like' 'back' 'want' 'know' 'come']\n"
     ]
    }
   ],
   "source": [
    "num_terms = 20\n",
    "for i in range(num_topics):\n",
    "    print(\"___topic \" + str(i) + \"___\")\n",
    "    topicName = \"topic\" + str(i)\n",
    "    weightedlist = topic_weights.get(topicName).sort_values()[-num_terms:]\n",
    "    print(weightedlist.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the association between our documents (individual nursery rhymes or other data samples) and our topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>topic10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>funny-how-i-use</th>\n",
       "      <td>-0.028158</td>\n",
       "      <td>-0.022191</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>-0.022046</td>\n",
       "      <td>-0.001228</td>\n",
       "      <td>0.011051</td>\n",
       "      <td>1.190752e-02</td>\n",
       "      <td>-0.004582</td>\n",
       "      <td>0.005543</td>\n",
       "      <td>-0.007585</td>\n",
       "      <td>-0.008984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early-storm-warning</th>\n",
       "      <td>-0.029878</td>\n",
       "      <td>-0.029482</td>\n",
       "      <td>-0.009811</td>\n",
       "      <td>-0.017737</td>\n",
       "      <td>-0.007365</td>\n",
       "      <td>-0.002144</td>\n",
       "      <td>-1.110060e-02</td>\n",
       "      <td>-0.000906</td>\n",
       "      <td>-0.008606</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>-0.003393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>death</th>\n",
       "      <td>-0.037291</td>\n",
       "      <td>-0.031628</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>-0.016238</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.008248</td>\n",
       "      <td>3.574383e-03</td>\n",
       "      <td>-0.014906</td>\n",
       "      <td>-0.025326</td>\n",
       "      <td>-0.002405</td>\n",
       "      <td>0.004059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bringing</th>\n",
       "      <td>-0.047897</td>\n",
       "      <td>-0.034436</td>\n",
       "      <td>-0.008644</td>\n",
       "      <td>-0.020683</td>\n",
       "      <td>-0.003317</td>\n",
       "      <td>-0.012378</td>\n",
       "      <td>-2.091547e-02</td>\n",
       "      <td>-0.000568</td>\n",
       "      <td>-0.017016</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>-0.005063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currently-using</th>\n",
       "      <td>-0.025684</td>\n",
       "      <td>-0.025513</td>\n",
       "      <td>-0.001790</td>\n",
       "      <td>-0.022443</td>\n",
       "      <td>-0.003708</td>\n",
       "      <td>-0.000975</td>\n",
       "      <td>6.374709e-07</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>-0.000923</td>\n",
       "      <td>-0.000617</td>\n",
       "      <td>-0.002917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cant-wait-to-bombard</th>\n",
       "      <td>-0.000959</td>\n",
       "      <td>0.035453</td>\n",
       "      <td>-0.007797</td>\n",
       "      <td>0.197418</td>\n",
       "      <td>-0.036704</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>7.349593e-02</td>\n",
       "      <td>-0.134875</td>\n",
       "      <td>0.032211</td>\n",
       "      <td>-0.073422</td>\n",
       "      <td>-0.051058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad-decisions-by</th>\n",
       "      <td>-0.024820</td>\n",
       "      <td>-0.018542</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>-0.006362</td>\n",
       "      <td>-0.008628</td>\n",
       "      <td>0.013516</td>\n",
       "      <td>1.003601e-02</td>\n",
       "      <td>-0.011656</td>\n",
       "      <td>-0.008455</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>-0.015133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accept-the-fact-that</th>\n",
       "      <td>-0.011093</td>\n",
       "      <td>0.007285</td>\n",
       "      <td>0.029115</td>\n",
       "      <td>-0.037244</td>\n",
       "      <td>0.011337</td>\n",
       "      <td>0.036310</td>\n",
       "      <td>4.507096e-02</td>\n",
       "      <td>-0.011213</td>\n",
       "      <td>0.087117</td>\n",
       "      <td>-0.037293</td>\n",
       "      <td>-0.084415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake-or-genuine</th>\n",
       "      <td>-0.029604</td>\n",
       "      <td>-0.020347</td>\n",
       "      <td>-0.010615</td>\n",
       "      <td>-0.007916</td>\n",
       "      <td>-0.007046</td>\n",
       "      <td>-0.002101</td>\n",
       "      <td>-6.563337e-05</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>-0.002543</td>\n",
       "      <td>0.008792</td>\n",
       "      <td>-0.005022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33-her-knittingneedles-clicked-and</th>\n",
       "      <td>-0.025570</td>\n",
       "      <td>-0.027297</td>\n",
       "      <td>-0.019508</td>\n",
       "      <td>0.031678</td>\n",
       "      <td>-0.025003</td>\n",
       "      <td>-0.036804</td>\n",
       "      <td>6.669356e-02</td>\n",
       "      <td>0.111116</td>\n",
       "      <td>0.027868</td>\n",
       "      <td>0.045590</td>\n",
       "      <td>-0.023453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      topic0    topic1    topic2    topic3  \\\n",
       "funny-how-i-use                    -0.028158 -0.022191  0.002462 -0.022046   \n",
       "early-storm-warning                -0.029878 -0.029482 -0.009811 -0.017737   \n",
       "death                              -0.037291 -0.031628  0.003428 -0.016238   \n",
       "bringing                           -0.047897 -0.034436 -0.008644 -0.020683   \n",
       "currently-using                    -0.025684 -0.025513 -0.001790 -0.022443   \n",
       "cant-wait-to-bombard               -0.000959  0.035453 -0.007797  0.197418   \n",
       "bad-decisions-by                   -0.024820 -0.018542  0.002438 -0.006362   \n",
       "accept-the-fact-that               -0.011093  0.007285  0.029115 -0.037244   \n",
       "fake-or-genuine                    -0.029604 -0.020347 -0.010615 -0.007916   \n",
       "33-her-knittingneedles-clicked-and -0.025570 -0.027297 -0.019508  0.031678   \n",
       "\n",
       "                                      topic4    topic5        topic6  \\\n",
       "funny-how-i-use                    -0.001228  0.011051  1.190752e-02   \n",
       "early-storm-warning                -0.007365 -0.002144 -1.110060e-02   \n",
       "death                               0.000583  0.008248  3.574383e-03   \n",
       "bringing                           -0.003317 -0.012378 -2.091547e-02   \n",
       "currently-using                    -0.003708 -0.000975  6.374709e-07   \n",
       "cant-wait-to-bombard               -0.036704  0.002897  7.349593e-02   \n",
       "bad-decisions-by                   -0.008628  0.013516  1.003601e-02   \n",
       "accept-the-fact-that                0.011337  0.036310  4.507096e-02   \n",
       "fake-or-genuine                    -0.007046 -0.002101 -6.563337e-05   \n",
       "33-her-knittingneedles-clicked-and -0.025003 -0.036804  6.669356e-02   \n",
       "\n",
       "                                      topic7    topic8    topic9   topic10  \n",
       "funny-how-i-use                    -0.004582  0.005543 -0.007585 -0.008984  \n",
       "early-storm-warning                -0.000906 -0.008606  0.001513 -0.003393  \n",
       "death                              -0.014906 -0.025326 -0.002405  0.004059  \n",
       "bringing                           -0.000568 -0.017016  0.001154 -0.005063  \n",
       "currently-using                    -0.002958 -0.000923 -0.000617 -0.002917  \n",
       "cant-wait-to-bombard               -0.134875  0.032211 -0.073422 -0.051058  \n",
       "bad-decisions-by                   -0.011656 -0.008455  0.001252 -0.015133  \n",
       "accept-the-fact-that               -0.011213  0.087117 -0.037293 -0.084415  \n",
       "fake-or-genuine                     0.005250 -0.002543  0.008792 -0.005022  \n",
       "33-her-knittingneedles-clicked-and  0.111116  0.027868  0.045590 -0.023453  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_topic_vectors_df = pd.DataFrame(svd_topic_vectors, index=document_labels, columns=labels)\n",
    "svd_topic_vectors_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can sort by importance for a particular topic. \n",
    "\n",
    "Try changing the topic that you are sorting by and see if you can see a correspondence between the most important words in the topic with the lyrics of the nursery rhyme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>topic10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aint-no-telling-when</th>\n",
       "      <td>0.194233</td>\n",
       "      <td>0.455978</td>\n",
       "      <td>-0.262186</td>\n",
       "      <td>-0.078836</td>\n",
       "      <td>-0.187010</td>\n",
       "      <td>-0.175662</td>\n",
       "      <td>0.040292</td>\n",
       "      <td>0.258332</td>\n",
       "      <td>-0.064488</td>\n",
       "      <td>0.024369</td>\n",
       "      <td>-0.076291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>any-else-feel-like</th>\n",
       "      <td>0.148987</td>\n",
       "      <td>0.422396</td>\n",
       "      <td>0.087429</td>\n",
       "      <td>-0.086950</td>\n",
       "      <td>-0.193809</td>\n",
       "      <td>-0.113413</td>\n",
       "      <td>-0.119443</td>\n",
       "      <td>-0.031161</td>\n",
       "      <td>-0.134578</td>\n",
       "      <td>0.212705</td>\n",
       "      <td>-0.142474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aint-even-going</th>\n",
       "      <td>0.151649</td>\n",
       "      <td>0.413920</td>\n",
       "      <td>-0.335780</td>\n",
       "      <td>-0.142833</td>\n",
       "      <td>0.068669</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>-0.002969</td>\n",
       "      <td>-0.128330</td>\n",
       "      <td>-0.004203</td>\n",
       "      <td>0.288331</td>\n",
       "      <td>-0.063719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everyone-is-not</th>\n",
       "      <td>0.172524</td>\n",
       "      <td>0.408512</td>\n",
       "      <td>0.046856</td>\n",
       "      <td>0.073188</td>\n",
       "      <td>-0.141290</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.043685</td>\n",
       "      <td>0.019006</td>\n",
       "      <td>-0.290729</td>\n",
       "      <td>-0.106959</td>\n",
       "      <td>0.004850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as-mondays-go-feels</th>\n",
       "      <td>0.152911</td>\n",
       "      <td>0.398118</td>\n",
       "      <td>-0.117758</td>\n",
       "      <td>-0.012031</td>\n",
       "      <td>-0.171491</td>\n",
       "      <td>-0.092208</td>\n",
       "      <td>0.022053</td>\n",
       "      <td>-0.021861</td>\n",
       "      <td>-0.248067</td>\n",
       "      <td>-0.061756</td>\n",
       "      <td>-0.036767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally-got-you</th>\n",
       "      <td>0.469727</td>\n",
       "      <td>-0.244443</td>\n",
       "      <td>-0.008688</td>\n",
       "      <td>-0.011306</td>\n",
       "      <td>0.009786</td>\n",
       "      <td>-0.016822</td>\n",
       "      <td>-0.063355</td>\n",
       "      <td>-0.020652</td>\n",
       "      <td>-0.065195</td>\n",
       "      <td>0.089631</td>\n",
       "      <td>-0.017027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>after-that-i-pray</th>\n",
       "      <td>0.429810</td>\n",
       "      <td>-0.250555</td>\n",
       "      <td>-0.019274</td>\n",
       "      <td>0.026607</td>\n",
       "      <td>-0.007605</td>\n",
       "      <td>-0.036482</td>\n",
       "      <td>-0.061203</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>-0.045355</td>\n",
       "      <td>0.053723</td>\n",
       "      <td>0.003384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cant-get-enough-please</th>\n",
       "      <td>0.548134</td>\n",
       "      <td>-0.260125</td>\n",
       "      <td>0.025948</td>\n",
       "      <td>-0.015257</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.040488</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>-0.058717</td>\n",
       "      <td>0.053853</td>\n",
       "      <td>-0.015599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause-now-im-getting</th>\n",
       "      <td>0.556160</td>\n",
       "      <td>-0.290220</td>\n",
       "      <td>0.021855</td>\n",
       "      <td>-0.011897</td>\n",
       "      <td>0.030056</td>\n",
       "      <td>-0.005711</td>\n",
       "      <td>-0.051629</td>\n",
       "      <td>-0.013786</td>\n",
       "      <td>-0.056356</td>\n",
       "      <td>0.054215</td>\n",
       "      <td>0.008182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baby-youre-the-man</th>\n",
       "      <td>0.625248</td>\n",
       "      <td>-0.318580</td>\n",
       "      <td>0.008539</td>\n",
       "      <td>-0.010907</td>\n",
       "      <td>0.047836</td>\n",
       "      <td>-0.007679</td>\n",
       "      <td>-0.053777</td>\n",
       "      <td>0.005357</td>\n",
       "      <td>-0.076303</td>\n",
       "      <td>0.049761</td>\n",
       "      <td>0.033726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27081 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          topic0    topic1    topic2    topic3    topic4  \\\n",
       "aint-no-telling-when    0.194233  0.455978 -0.262186 -0.078836 -0.187010   \n",
       "any-else-feel-like      0.148987  0.422396  0.087429 -0.086950 -0.193809   \n",
       "aint-even-going         0.151649  0.413920 -0.335780 -0.142833  0.068669   \n",
       "everyone-is-not         0.172524  0.408512  0.046856  0.073188 -0.141290   \n",
       "as-mondays-go-feels     0.152911  0.398118 -0.117758 -0.012031 -0.171491   \n",
       "...                          ...       ...       ...       ...       ...   \n",
       "finally-got-you         0.469727 -0.244443 -0.008688 -0.011306  0.009786   \n",
       "after-that-i-pray       0.429810 -0.250555 -0.019274  0.026607 -0.007605   \n",
       "cant-get-enough-please  0.548134 -0.260125  0.025948 -0.015257  0.038900   \n",
       "cause-now-im-getting    0.556160 -0.290220  0.021855 -0.011897  0.030056   \n",
       "baby-youre-the-man      0.625248 -0.318580  0.008539 -0.010907  0.047836   \n",
       "\n",
       "                          topic5    topic6    topic7    topic8    topic9  \\\n",
       "aint-no-telling-when   -0.175662  0.040292  0.258332 -0.064488  0.024369   \n",
       "any-else-feel-like     -0.113413 -0.119443 -0.031161 -0.134578  0.212705   \n",
       "aint-even-going         0.000991 -0.002969 -0.128330 -0.004203  0.288331   \n",
       "everyone-is-not         0.169900  0.043685  0.019006 -0.290729 -0.106959   \n",
       "as-mondays-go-feels    -0.092208  0.022053 -0.021861 -0.248067 -0.061756   \n",
       "...                          ...       ...       ...       ...       ...   \n",
       "finally-got-you        -0.016822 -0.063355 -0.020652 -0.065195  0.089631   \n",
       "after-that-i-pray      -0.036482 -0.061203 -0.000112 -0.045355  0.053723   \n",
       "cant-get-enough-please  0.005578 -0.040488  0.000605 -0.058717  0.053853   \n",
       "cause-now-im-getting   -0.005711 -0.051629 -0.013786 -0.056356  0.054215   \n",
       "baby-youre-the-man     -0.007679 -0.053777  0.005357 -0.076303  0.049761   \n",
       "\n",
       "                         topic10  \n",
       "aint-no-telling-when   -0.076291  \n",
       "any-else-feel-like     -0.142474  \n",
       "aint-even-going        -0.063719  \n",
       "everyone-is-not         0.004850  \n",
       "as-mondays-go-feels    -0.036767  \n",
       "...                          ...  \n",
       "finally-got-you        -0.017027  \n",
       "after-that-i-pray       0.003384  \n",
       "cant-get-enough-please -0.015599  \n",
       "cause-now-im-getting    0.008182  \n",
       "baby-youre-the-man      0.033726  \n",
       "\n",
       "[27081 rows x 11 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_topic_vectors_df.sort_values(by=['topic1'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "**Task 1:** Compare this notebook to the Bag of Words + LDA topic modelling notebook. What differences do you see? Are the topics any better or more intelligible using this notebook?\n",
    "\n",
    "**Task 2:** Change the [number of topics](#num-topics) generated by the topic modellig algorithm. How does that effect the topics? Is using more or less topics better?\n",
    "\n",
    "**Task 3:** Adjust the n-gram parameters [in the cell that defines the TF-IDF vectorizer](#vectorizer), i.e. make the range `1,2` if you want to include individual words and bi-grams, or `2,3` if you want to use bi-grams and tri-grams. How does that effect the topics?\n",
    "\n",
    "**Task 4:** Once you have done that, try loading in a different dataset and try out topic modelling on that. There is a [dataset of limericks](https://git.arts.ac.uk/tbroad/limerick-dataset), a [dataset of haikus](https://git.arts.ac.uk/tbroad/haiku-dataset), and a [dataset of EPL fan chants](https://git.arts.ac.uk/tbroad/SFW-EPL-fan-chants-dataset) (nursery rhymes for grown men) that have been created to be in the same format as the nursery rhymes dataset. Simply download them (unzip if you need to) and move the dataset folder into the folder `../data/my-data` and [edit the path](#load-data) for the new dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
