{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-2 on custom datasets\n",
    "\n",
    "In this notebook we will see how we can fine-tune a transformer model on our own custom datasets. Here we will be using pre-trained transformer models, which are an advanced neural network architecture primarly used for text understanding and generation. We will be covering transformers in more detail in Term 2 (AI for Media). \n",
    "\n",
    "To run this code you will need to install the following `transformers` library from [huggingface](https://huggingface.co/docs/transformers/index), this allows us to use and fine-tune many pre-trained transformer models. \n",
    "\n",
    "This code is originally [sourced from here](https://github.com/mf1024/Transformers/blob/master/Fine-tuning%20GPT2-medium%20in%20PyTorch.ipynb), and has been adapted to be clearer and easier to load in different kinds of datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     --------- ----------------------------- 30.7/123.5 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 123.5/123.5 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from transformers) (23.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/84/4d/82704d1ab9290b03da94e6425f5e87396b999fd7eb8e08f3a92c158402bf/PyYAML-6.0.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/9f/90/a6821e7757d2db194c16cbca78c80e206f30f6cc62c7f15fb27428f8c6dd/tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/4e/96/f4ee4434d8b6452fe7d5d44df2e72d1c6b2add1c3a5fb5c81aae83cb90c6/safetensors-0.4.1-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp39-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from requests->transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/7.9 MB 7.0 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.8/7.9 MB 9.5 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.1/7.9 MB 8.7 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.5/7.9 MB 8.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.9/7.9 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.1/7.9 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.5/7.9 MB 8.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.8/7.9 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.4/7.9 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.8/7.9 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.2/7.9 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.5/7.9 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.1/7.9 MB 9.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.4/7.9 MB 9.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.2/7.9 MB 9.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.7/7.9 MB 9.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.0/7.9 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.5/7.9 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 9.2 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 311.7/311.7 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.8/152.8 kB ? eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp39-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 277.8/277.8 kB 8.6 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 11.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 16.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.0/2.2 MB 15.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 15.5 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, pyyaml, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.19.4 pyyaml-6.0.1 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.35.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets do some imports and set our device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ROG\\anaconda3\\envs\\stem\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now need to download the GPT2 models. This is nearly 2GB so it may take some time to download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.78MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.88MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 1.71MB/s]\n",
      "config.json: 100%|██████████| 718/718 [00:00<00:00, 476kB/s]\n",
      "model.safetensors: 100%|██████████| 1.52G/1.52G [03:18<00:00, 7.65MB/s]\n",
      "generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 49.1kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our code for sampling from our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_from_top(probs, n=5):\n",
    "    ind = np.argpartition(probs, -n)[-n:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
    "    choice = np.random.choice(n, 1, p = top_prob)\n",
    "    token_id = ind[choice][0]\n",
    "    return int(token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset classes\n",
    "\n",
    "Here we define our dataset classes (by inheriting from the [PyTorch Dataset class](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)). There is a TXTDataset class, that automatically loads in a dataset of `.txt` files in a folder (such as the nursery rhymes dataset). There is also a TSVDataset class. This will allow you to load in data from a `.tsv` file. Change the `dataset_path` parameter when initialising the class to load in your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TXTDataset(Dataset):\n",
    "    def __init__(self, dataset_path = '../data/nursery-rhymes'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_list = []\n",
    "        self.end_of_text_token = \"<|endoftext|>\"\n",
    "        \n",
    "        for root, _, files in os.walk(dataset_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                        data_str = f.read()\n",
    "                        self.data_list.append(f'TEXT:{data_str}{self.end_of_text_token}')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data_list[item]\n",
    "    \n",
    "class TSVDataset(Dataset):\n",
    "    def __init__(self, dataset_path = '../data/lyric_data.tsv', data_row_index = 4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_list = []\n",
    "        self.end_of_text_token = \"<|endoftext|>\"\n",
    "        \n",
    "        with open(dataset_path) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter='\\t')\n",
    "            \n",
    "            x = 0\n",
    "            for row in csv_reader:\n",
    "                data_str = f\"TEXT:{row[data_row_index]}{self.end_of_text_token}\"\n",
    "                self.data_list.append(data_str)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data_list[item]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Depending on the size of your dataset, you will want to adjust the number of epochs you are training for. It will take a long time to process each epoch with a large dataset, so you will want to keep it low. But for a small dataset, training for a small number of epochs will not be sufficient to change the output from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 3e-5\n",
    "WARMUP_STEPS = 5000\n",
    "MAX_SEQ_LEN = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = TXTDataset(dataset_path = '../data/nursery-rhymes')\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\rog\\anaconda3\\envs\\stem\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "I will train the model and save the model weights after each epoch and then I will try to generate jokes with each version of the weight to see which performs the best.\n",
    "\n",
    "**Warning:** depending on the size of the dataset this can take **a very long time** to train. Make sure your laptop is plugged in while doing this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 started==============================\n",
      "EPOCH 1 started==============================\n",
      "EPOCH 2 started==============================\n",
      "EPOCH 3 started==============================\n",
      "EPOCH 4 started==============================\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps = -1)\n",
    "proc_seq_count = 0\n",
    "sum_loss = 0.0\n",
    "batch_count = 0\n",
    "\n",
    "tmp_text_tokens = None\n",
    "models_folder = \"ckpt/gpt2\"\n",
    "# if not os.path.exists(models_folder):\n",
    "#     os.mkdir(models_folder)\n",
    "\n",
    "if not os.path.exists(models_folder):\n",
    "    os.makedirs(models_folder, exist_ok=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
    "    \n",
    "    for idx, text_str in enumerate(data_loader):\n",
    "        \n",
    "        \n",
    "        text_tokens = torch.tensor(tokenizer.encode(text_str[0])).unsqueeze(0).to(device)\n",
    "        #Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
    "        if text_tokens.size()[1] > MAX_SEQ_LEN:\n",
    "            continue\n",
    "        \n",
    "        #The first joke sequence in the sequence\n",
    "        if not torch.is_tensor(tmp_text_tokens):\n",
    "            tmp_text_tokens = text_tokens\n",
    "            continue\n",
    "        else:\n",
    "            #The next joke does not fit in so we process the sequence and leave the last joke \n",
    "            #as the start for next sequence \n",
    "            if tmp_text_tokens.size()[1] + text_tokens.size()[1] > MAX_SEQ_LEN:\n",
    "                work_text_tokens = tmp_text_tokens\n",
    "                tmp_text_tokens = text_tokens\n",
    "            else:\n",
    "                #Add the joke to sequence, continue and try to add more\n",
    "                tmp_text_tokens = torch.cat([tmp_text_tokens, text_tokens[:,1:]], dim=1)\n",
    "                continue\n",
    "        ################## Sequence ready, process it trough the model ##################\n",
    "            \n",
    "        outputs = model(work_text_tokens, labels=work_text_tokens)\n",
    "        loss, logits = outputs[:2]                        \n",
    "        loss.backward()\n",
    "        sum_loss = sum_loss + loss.detach().data\n",
    "                       \n",
    "        proc_seq_count = proc_seq_count + 11\n",
    "        if proc_seq_count == BATCH_SIZE:\n",
    "            proc_seq_count = 0    \n",
    "            batch_count += 1\n",
    "            optimizer.step()\n",
    "            scheduler.step() \n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "\n",
    "        if batch_count == 100:\n",
    "            print(f\"sum loss {sum_loss}\")\n",
    "            batch_count = 0\n",
    "            sum_loss = 0.0\n",
    "    \n",
    "    # Store the model after each epoch to compare the performance of them\n",
    "    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_medium_finetuned_{epoch}.pt\"))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text from the model:\n",
    "\n",
    "Here we will generated text from the model we have trained. If you have set `EPOCHS` for longer then change the parameter here for which checkpoint (defined in `MODEL_EPOCH`) that you want to load in and generate from. If the model is not sufficiently mimicing your training data then you may need to train for more epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT:\n",
      "\n",
      "The first time we saw him in a video, he had been playing with his dog, and he was wearing a black T-shirt and black shorts.\n",
      "\n",
      "\"I was like, 'What's going on?'\" he said. \"And he said, 'I'm not wearing a shirt, I just want to play with my dog.'\"\n",
      "\n",
      "\n",
      "The video shows him playing with his dog and then walking away.\n",
      "\n",
      "He said he was wearing his shirt and shorts because he was afraid he would be arrested. He said he had no idea what the police were doing.\n",
      "\n",
      "\n",
      "\"It's just crazy, I'm like, 'What is going on?'\"\n",
      "\n",
      "Police said he was arrested and charged with disorderly conduct and obstructing a peace officer.\n",
      "\n",
      "The incident was caught on camera and posted on YouTube.\n",
      "\n",
      "The video has since been taken down.\n",
      "\n",
      "Police said they were investigating the case, but said they had not received any complaints from the public.\n",
      "\n",
      "The video was taken down after it was posted.\n",
      "\n",
      "Copyright 2017 by WKMG ClickOrlando - All rights reserved.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "MODEL_EPOCH = 4\n",
    "models_folder = \"ckpt/gpt2\"\n",
    "model_path = os.path.join(models_folder, f\"gpt2_medium_finetuned_{MODEL_EPOCH}.pt\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval()\n",
    "    \n",
    "with torch.no_grad():\n",
    "   \n",
    "        for idx in range(1):\n",
    "        \n",
    "            text_finished = False\n",
    "            cur_ids = torch.tensor(tokenizer.encode(\"TEXT:\")).unsqueeze(0).to(device)\n",
    "\n",
    "            for i in range(300):\n",
    "                outputs = model(cur_ids, labels=cur_ids)\n",
    "                loss, logits = outputs[:2]\n",
    "                softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n",
    "                if i < 3:\n",
    "                    n = 20\n",
    "                else:\n",
    "                    n = 3\n",
    "                next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) select the next word\n",
    "                cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n",
    "\n",
    "                if next_token_id in tokenizer.encode('<|endoftext|>'):\n",
    "                    text_finished = True\n",
    "                    break\n",
    "\n",
    "            output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "            output_text = tokenizer.decode(output_list)\n",
    "            print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
